<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.23.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="神经网络基础神经网络深度学习神经网络就是大脑仿生，数据从输入到输出经过一层一层的神经元产生预测值的过程就是前向传播（也叫正向传播）。 前向传播涉及到人工神经元是如何工作的（也就是神经元的初始化、激活函数），神经网络如何搭建，权重参数计算、数据形如何状变化。千里之行始于足下，我们一起进入深度学习的知识海洋吧。 神经网络概念什么是神经网络人工神经网络（Artificial Neural Network">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络基础">
<meta property="og:url" content="http://example.com/2025/05/14/a-004-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="李响，欢迎你回来">
<meta property="og:description" content="神经网络基础神经网络深度学习神经网络就是大脑仿生，数据从输入到输出经过一层一层的神经元产生预测值的过程就是前向传播（也叫正向传播）。 前向传播涉及到人工神经元是如何工作的（也就是神经元的初始化、激活函数），神经网络如何搭建，权重参数计算、数据形如何状变化。千里之行始于足下，我们一起进入深度学习的知识海洋吧。 神经网络概念什么是神经网络人工神经网络（Artificial Neural Network">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/NN_base/08.png">
<meta property="og:image" content="http://example.com/images/NN_base/07.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20220313130303542.png">
<meta property="og:image" content="http://example.com/images/NN_base/09.png">
<meta property="og:image" content="http://example.com/images/NN_base/10.png">
<meta property="og:image" content="http://example.com/images/NN_base/11.png">
<meta property="og:image" content="http://example.com/images/NN_base/21.png">
<meta property="og:image" content="http://example.com/images/NN_base/1663472645904.png">
<meta property="og:image" content="http://example.com/images/NN_base/25.png">
<meta property="og:image" content="http://example.com/images/NN_base/23.png">
<meta property="og:image" content="http://example.com/images/NN_base/1663472675509.png">
<meta property="og:image" content="http://example.com/images/NN_base/24.png">
<meta property="og:image" content="http://example.com/images/NN_base/26.png">
<meta property="og:image" content="http://example.com/images/NN_base/1663472703595.png">
<meta property="og:image" content="http://example.com/images/NN_base/1734057447434.png">
<meta property="og:image" content="http://example.com/images/NN_base/29.png">
<meta property="og:image" content="http://example.com/images/NN_base/30.png">
<meta property="og:image" content="http://example.com/images/NN_base/31.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20220313133601678.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20220313141138281.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-1.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-2.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-3.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-4.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-5.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-6.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-7.png">
<meta property="og:image" content="http://example.com/images/NN_base/1735117379356.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-8.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-9.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-10.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-11.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-12.png">
<meta property="og:image" content="http://example.com/images/NN_base/03-13.png">
<meta property="og:image" content="http://example.com/images/NN_base/12.png">
<meta property="og:image" content="http://example.com/images/NN_base/04-05.png">
<meta property="og:image" content="http://example.com/images/NN_base/13.png">
<meta property="og:image" content="http://example.com/images/NN_base/14.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20210129103846076.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20210129104612230.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20210129104646474.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20210129104718140.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20210129104756053.png">
<meta property="og:image" content="http://example.com/images/NN_base/50.png">
<meta property="og:image" content="http://example.com/images/NN_base/37.png">
<meta property="og:image" content="http://example.com/images/NN_base/1735177998337.png">
<meta property="og:image" content="http://example.com/images/NN_base/1734144867552.png">
<meta property="og:image" content="http://example.com/images/NN_base/42.png">
<meta property="og:image" content="http://example.com/images/NN_base/04-01.png">
<meta property="og:image" content="http://example.com/images/NN_base/04-02.png">
<meta property="og:image" content="http://example.com/images/NN_base/04-03.png">
<meta property="og:image" content="http://example.com/images/NN_base/04-04.png">
<meta property="og:image" content="http://example.com/images/NN_base/05-01.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20220517141847967.png">
<meta property="og:image" content="http://example.com/images/NN_base/image-20220517143935599.png">
<meta property="og:image" content="http://example.com/images/NN_base/48.png">
<meta property="og:image" content="http://example.com/images/NN_base/1734255905857.png">
<meta property="og:image" content="http://example.com/images/NN_base/1734256199295.png">
<meta property="og:image" content="http://example.com/images/NN_base/1734256384162.png">
<meta property="article:published_time" content="2025-05-14T06:58:17.130Z">
<meta property="article:modified_time" content="2025-05-18T16:10:37.423Z">
<meta property="article:author" content="李响">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/NN_base/08.png">


<link rel="canonical" href="http://example.com/2025/05/14/a-004-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2025/05/14/a-004-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","path":"2025/05/14/a-004-神经网络基础/","title":"神经网络基础"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>神经网络基础 | 李响，欢迎你回来</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">李响，欢迎你回来</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">本网站是我的个人博客，主要用于记录我个人学习的内容以及一些杂谈、心情记录、文摘等</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">6</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">7</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.1.</span> <span class="nav-text">神经网络概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">什么是神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">如何构建神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%86%85%E9%83%A8%E7%8A%B6%E6%80%81%E5%80%BC%E5%92%8C%E6%BF%80%E6%B4%BB%E5%80%BC"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">神经网络内部状态值和激活值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%A0%E7%B4%A0%E7%90%86%E8%A7%A3"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">网络非线性因素理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">常见激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Sigmoid-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.1.</span> <span class="nav-text">Sigmoid 激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Tanh-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.2.</span> <span class="nav-text">Tanh 激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.3.</span> <span class="nav-text">ReLU 激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SoftMax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.4.</span> <span class="nav-text">SoftMax激活函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">如何选择激活函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.1.3.</span> <span class="nav-text">参数初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">常见参数初始化方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">如何选择参数初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%92%8C%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="nav-number">1.1.4.</span> <span class="nav-text">神经网络搭建和参数计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">构建神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE%E5%BD%A2%E7%8A%B6%E5%8F%98%E5%8C%96"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">观察数据形状变化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">模型参数计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.4.4.</span> <span class="nav-text">查看模型参数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A6%82%E5%BF%B5"><span class="nav-number">1.2.1.</span> <span class="nav-text">损失函数概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">分类任务损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">多分类任务损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">二分类任务损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.</span> <span class="nav-text">回归任务损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MAE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">MAE损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MSE%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">MSE损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Smooth-L1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">Smooth L1损失函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%9B%9E%E9%A1%BE"><span class="nav-number">1.3.1.</span> <span class="nav-text">梯度下降算法回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88BP%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">1.3.2.</span> <span class="nav-text">反向传播（BP算法）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A6%82%E5%BF%B5"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">反向传播概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%A6%E8%A7%A3"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">反向传播详解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.3.3.</span> <span class="nav-text">梯度下降优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E7%AE%97%E6%B3%95Momentum"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">动量算法Momentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaGrad"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">AdaGrad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSProp"><span class="nav-number">1.3.3.4.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">1.3.3.5.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">1.3.3.6.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">学习率衰减优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%BC%98%E5%8C%96"><span class="nav-number">1.4.1.</span> <span class="nav-text">为什么要进行学习率优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%89%E9%97%B4%E9%9A%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="nav-number">1.4.2.</span> <span class="nav-text">等间隔学习率衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A%E9%97%B4%E9%9A%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="nav-number">1.4.3.</span> <span class="nav-text">指定间隔学习率衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%89%E6%8C%87%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="nav-number">1.4.4.</span> <span class="nav-text">按指数学习率衰减</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="nav-number">1.4.5.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.5.</span> <span class="nav-text">正则化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.5.1.</span> <span class="nav-text">什么是正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.5.2.</span> <span class="nav-text">Dropout正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E6%AD%A3%E5%88%99%E5%8C%96-Batch-Normalization"><span class="nav-number">1.5.3.</span> <span class="nav-text">批量归一正则化(Batch Normalization)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%8B%E6%9C%BA%E4%BB%B7%E6%A0%BC%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B"><span class="nav-number">1.6.</span> <span class="nav-text">手机价格分类案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="nav-number">1.6.1.</span> <span class="nav-text">案例需求分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.6.2.</span> <span class="nav-text">构建数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.6.3.</span> <span class="nav-text">构建分类网络模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">1.6.4.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">1.6.5.</span> <span class="nav-text">模型评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="nav-number">1.6.6.</span> <span class="nav-text">网络性能优化</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">李响</p>
  <div class="site-description" itemprop="description">愿你我都做生活的高手</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/a-004-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="李响">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李响，欢迎你回来">
      <meta itemprop="description" content="愿你我都做生活的高手">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="神经网络基础 | 李响，欢迎你回来">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          神经网络基础
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-14 14:58:17" itemprop="dateCreated datePublished" datetime="2025-05-14T14:58:17+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-19 00:10:37" itemprop="dateModified" datetime="2025-05-19T00:10:37+08:00">2025-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">技术分享</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>深度学习神经网络就是大脑仿生，数据从输入到输出经过一层一层的神经元产生预测值的过程就是前向传播（也叫正向传播）。</p>
<p>前向传播涉及到人工神经元是如何工作的（也就是神经元的初始化、激活函数），神经网络如何搭建，权重参数计算、数据形如何状变化。千里之行始于足下，我们一起进入深度学习的知识海洋吧。</p>
<h3 id="神经网络概念"><a href="#神经网络概念" class="headerlink" title="神经网络概念"></a>神经网络概念</h3><h4 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h4><p>人工神经网络（Artificial Neural Network， 简写为<strong>ANN</strong>）也简称为神经网络（NN），是一种模仿生物神经网络结构和功能的<strong>计算模型</strong>。它由多个互相连接的人工神经元（也称为节点）构成，可以用于处理和学习复杂的数据模式，尤其适合解决非线性问题。人工神经网络是机器学习中的一个重要模型，尤其在深度学习领域中得到了广泛应用。</p>
<p>人脑可以看做是一个生物神经网络，由众多的<strong>神经元</strong>连接而成。各个神经元传递复杂的电信号，树突接收到<strong>输入信号</strong>，然后对信号进行处理，通过轴突<strong>输出信号</strong>。</p>
<p>当电信号通过树突进入到细胞核时，会逐渐聚集电荷。达到一定的电位后，细胞就会被激活，通过轴突发出电信号。</p>
<span id="more"></span>

<h4 id="如何构建神经网络"><a href="#如何构建神经网络" class="headerlink" title="如何构建神经网络"></a>如何构建神经网络</h4><p>神经网络是由多个神经元组成，构建神经网络就是在构建神经元。以下是神经网络中神经元的构建说明：</p>
<p><img src="/images/NN_base/08.png" alt="08"></p>
<p>这个流程就像，来源不同树突(树突都会有不同的权重)的信息, 进行的加权计算, 输入到细胞中做加和，再通过激活函数输出细胞值。</p>
<p>同一层的多个神经元可以看作是通过并行计算来处理相同的输入数据，学习输入数据的不同特征。每个神经元可能会关注输入数据中的不同部分，从而捕捉到数据的不同属性。</p>
<p>接下来，我们使用多个神经元来构建神经网络，相邻层之间的神经元相互连接，并给每一个连接分配一个强度，如下图所示：</p>
<p><img src="/images/NN_base/07.png" alt="07"></p>
<p>神经网络中信息只向一个方向移动，即从输入节点向前移动，通过隐藏节点，再向输出节点移动。其中的基本部分是:</p>
<ol>
<li><strong>输入层（Input Layer）</strong>: 即输入x的那一层（如图像、文本、声音等）。每个输入特征对应一个神经元。输入层将数据传递给下一层的神经元。</li>
<li><strong>输出层（Output Layer）</strong>: 即输出y的那一层。输出层的神经元根据网络的任务（回归、分类等）生成最终的预测结果。</li>
<li><strong>隐藏层（Hidden Layers）</strong>: 输入层和输出层之间都是隐藏层，神经网络的“深度”通常由隐藏层的数量决定。隐藏层的神经元通过加权和激活函数处理输入，并将结果传递到下一层。</li>
</ol>
<p>&#x3D;&#x3D;特点是：&#x3D;&#x3D;</p>
<ul>
<li>同一层的神经元之间没有连接</li>
<li>第N层的每个神经元和第N-1层的所有神经元相连（这就是Fully Connected的含义)，这就是<strong>全连接神经网络（FCNN）</strong></li>
<li>全连接神经网络接收的样本数据是<strong>二维的</strong>，数据在每一层之间需要以二维的形式传递</li>
<li>第N-1层神经元的输出就是第N层神经元的输入</li>
<li>每个连接都有一个权重值（w系数和b系数）</li>
</ul>
<h4 id="神经网络内部状态值和激活值"><a href="#神经网络内部状态值和激活值" class="headerlink" title="神经网络内部状态值和激活值"></a>神经网络内部状态值和激活值</h4><p><img src="/images/NN_base/image-20220313130303542.png" alt="image-20220313130303542"></p>
<p>每一个神经元工作时，<strong>前向传播</strong>会产生两个值，<strong>内部状态值（加权求和值）<strong>和</strong>激活值</strong>；<strong>反向传播</strong>时会产生<strong>激活值梯度</strong>和<strong>内部状态值梯度</strong>。</p>
<ul>
<li><p>内部状态值</p>
<ul>
<li>神经元或隐藏单元的内部存储值，它反映了当前神经元接收到的输入、历史信息以及网络内部的权重计算结果。</li>
<li>每个输入$$x_i$$都有一个与之相乘的权重$$w_i​$$，表示每个输入信号的重要性。</li>
<li>z&#x3D;w⋅x+b<ul>
<li>w：权重矩阵</li>
<li>x：输入值</li>
<li>b：偏置</li>
</ul>
</li>
</ul>
</li>
<li><p>激活值</p>
<ul>
<li>通过激活函数（如 ReLU、Sigmoid、Tanh）对内部状态值进行非线性变换后得到的结果。激活值决定了当前神经元的输出。</li>
<li>a&#x3D;f(z)<ul>
<li>f：激活函数</li>
<li>z：内部状态值</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>通过控制每个神经元的内部状态值、激活值的大小；每一层的内部状态值的方差、每一层的激活值的方差可让整个神经网络工作的更好。</p>
<p>所以下面两个小结，我们将要学习神经元的激活函数，神经元的权重初始化。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="网络非线性因素理解"><a href="#网络非线性因素理解" class="headerlink" title="网络非线性因素理解"></a>网络非线性因素理解</h4><blockquote>
<ul>
<li><p>没有引入非线性因素的网络等价于使用一个线性模型来拟合</p>
</li>
<li><p>通过给网络输出增加激活函数, 实现引入非线性因素, 使得网络模型可以逼近任意函数, 提升网络对复杂问题的拟合能力</p>
</li>
</ul>
</blockquote>
<p><strong>激活函数</strong>用于对每层的<strong>输出数据进行变换</strong>, 进而为整个网络注入了<strong>非线性因素</strong>。此时, 神经网络就可以拟合各种曲线。如果不使用激活函数，整个网络虽然看起来复杂，其本质还相当于一种<strong>线性模型</strong>，如下公式所示:</p>
<p>​		<img src="/images/NN_base/09.png" alt=" "><br>​		<img src="/images/NN_base/10.png" alt=" "></p>
<p>另外通过图像可视化的形式理解：</p>
<p><a target="_blank" rel="noopener" href='http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.97420&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false'>神经网络可视化</a></p>
<p><img src="/images/NN_base/11.png" alt="11"></p>
<p>我们发现增加激活函数之后, 对于线性不可分的场景，神经网络的拟合能力更强。</p>
<h4 id="常见激活函数"><a href="#常见激活函数" class="headerlink" title="常见激活函数"></a>常见激活函数</h4><blockquote>
<p>激活函数主要用来向神经网络中加入非线性因素，以解决线性模型表达能力不足的问题，它对神经网络有着极其重要的作用。我们的网络参数在更新时，使用的反向传播算法（BP），这就要求我们的激活函数必须可微。</p>
</blockquote>
<h5 id="Sigmoid-激活函数"><a href="#Sigmoid-激活函数" class="headerlink" title="Sigmoid 激活函数"></a>Sigmoid 激活函数</h5><p>激活函数公式：</p>
<p><img src="/images/NN_base/21.png" alt="21"></p>
<p>激活函数求导公式：</p>
<p><img src="/images/NN_base/1663472645904.png" alt="1663472645904"></p>
<p>sigmoid 激活函数的函数图像如下:</p>
<p><img src="/images/NN_base/25.png" alt="25"></p>
<ul>
<li><p>从sigmoid函数图像可以得到，sigmoid 函数可以将<strong>任意的输入</strong>映射到 <strong>(0, 1)</strong> 之间，当输入的值大致在**&lt;-6或者&gt;6**时，意味着输入任何值得到的激活值都是差不多的，这样会丢失部分的信息。比如：输入100和输入10000经过 sigmoid的激活值几乎都是等于1的，但是输入的数据之间相差100倍的信息就丢失了。</p>
</li>
<li><p>对于sigmoid函数而言，输入值在**[-6, 6]<strong>之间输出值才会</strong>有明显差异**，输入值在**[-3, 3]<strong>之间才会</strong>有比较好的效果**</p>
</li>
<li><p>通过上述导数图像，我们发现<strong>导数数值范围是 (0, 0.25)</strong>，当输入的值**&lt;-6或者&gt;6<strong>时，sigmoid激活函数图像的</strong>导数接近为 0**，此时<strong>网络参数将更新极其缓慢，或者无法更新。</strong></p>
</li>
<li><p>一般来说，sigmoid网络在<strong>5层之内</strong>就会产生<strong>梯度消失</strong>现象。而且，该激活函数的激活值并不是以0为中心的，激活值总是偏向正数，导致梯度更新时，只会对某些特征产生相同方向的影响，所以在实践中这种激活函数使用的很少。<strong>sigmoid函数一般只用于二分类的输出层</strong>。</p>
</li>
</ul>
<p>在 PyTorch中使用sigmoid函数的示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">绘制激活函数图像时出现以下提示，需要将anaconda3/Lib/site-packages/torch/lib目录下的libiomp5md.dll文件删除</span></span><br><span class="line"><span class="string">OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建画布和坐标轴</span></span><br><span class="line">_, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 输入值x通过sigmoid函数转换成激活值y</span></span><br><span class="line">y = torch.sigmoid(x)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y)</span><br><span class="line">axes[<span class="number">0</span>].grid()</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Sigmoid 函数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.sigmoid(x).<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.detach():输入值x的ndarray数组</span></span><br><span class="line"><span class="comment"># x.grad:计算梯度，求导</span></span><br><span class="line">axes[<span class="number">1</span>].plot(x.detach(), x.grad)</span><br><span class="line">axes[<span class="number">1</span>].grid()</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Sigmoid 导数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h5 id="Tanh-激活函数"><a href="#Tanh-激活函数" class="headerlink" title="Tanh 激活函数"></a>Tanh 激活函数</h5><p>Tanh叫做双曲正切函数，其公式如下：</p>
<p><img src="/images/NN_base/23.png" alt="23"></p>
<p>激活函数求导公式: </p>
<p><img src="/images/NN_base/1663472675509.png" alt="1663472675509"></p>
<p>Tanh的函数图像、导数图像如下：</p>
<p>​						<img src="/images/NN_base/24.png" alt="24">  </p>
<ul>
<li><p>由上面的函数图像可以看到，Tanh函数将<strong>输入映射到(-1, 1)之间</strong>，图像以0为中心，激活值在0点对称，当输入的值大概**&lt;-3或者&gt;3** 时将被映射为-1或者1。<strong>其导数值范围 (0, 1)</strong>，当输入的值大概**&lt;-3或者&gt;3**时，其导数近似0。</p>
</li>
<li><p>与Sigmoid相比，它是<strong>以0为中心的</strong>，使得其收敛速度要比Sigmoid快，减少迭代次数。然而，从图中可以看出，Tanh两侧的导数也为0，同样会造成梯度消失。</p>
</li>
<li><p>若使用时可在<strong>隐藏层使用tanh函数</strong>，在<strong>输出层使用sigmoid函数</strong>。</p>
</li>
</ul>
<p>在 PyTorch 中使用tanh函数的示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line"> </span><br><span class="line">_, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>)</span><br><span class="line">y = torch.tanh(x)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y)</span><br><span class="line">axes[<span class="number">0</span>].grid()</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Tanh 函数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.tanh(x).<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].plot(x.detach(), x.grad)</span><br><span class="line">axes[<span class="number">1</span>].grid()</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Tanh 导数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h5 id="ReLU-激活函数"><a href="#ReLU-激活函数" class="headerlink" title="ReLU 激活函数"></a>ReLU 激活函数</h5><p>ReLU 激活函数公式如下：</p>
<p><img src="/images/NN_base/26.png" alt="26"></p>
<p>激活函数求导公式: </p>
<p><img src="/images/NN_base/1663472703595.png" alt="1663472703595"></p>
<p>ReLU 的函数图像、导数图像如下：</p>
<p><img src="/images/NN_base/1734057447434.png" alt="1734057447434"></p>
<ul>
<li>ReLU 激活函数将小于0的值映射为0，而大于0的值则保持不变，它更加重视正信号，而忽略负信号，这种激活函数运算更为简单，能够提高模型的训练效率。</li>
<li>当x&lt;0时，ReLU导数为0，而当x&gt;0时，则不存在饱和问题。所以，ReLU 能够在x&gt;0时保持梯度不衰减，从而缓解梯度消失问题。然而，随着训练的推进，部分输入会落入小于0区域，导致对应权重无法更新。这种现象被称为“神经元死亡”。</li>
<li>ReLU是目前&#x3D;&#x3D;最常用的激活函数&#x3D;&#x3D;。与sigmoid相比，RELU的优势是：<ul>
<li>采用sigmoid函数，计算量大（指数运算），反向传播求误差梯度时，计算量相对大；而采用Relu激活函数，整个过程的计算量节省很多 </li>
<li>sigmoid函数反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；而采用relu激活函数，当输入的值&gt;0时，梯度为1，不会出现梯度消失的情况 </li>
<li>Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生</li>
</ul>
</li>
</ul>
<p>在 PyTorch 中使用ReLU函数的示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line"></span><br><span class="line">_, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>)</span><br><span class="line">y = torch.relu(x)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y)</span><br><span class="line">axes[<span class="number">0</span>].grid()</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Tanh 函数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导数图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">20</span>, <span class="number">20</span>, <span class="number">1000</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.relu(x).<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].plot(x.detach(), x.grad)</span><br><span class="line">axes[<span class="number">1</span>].grid()</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Tanh 导数图像&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h5 id="SoftMax激活函数"><a href="#SoftMax激活函数" class="headerlink" title="SoftMax激活函数"></a>SoftMax激活函数</h5><p>softmax用于<strong>多分类</strong>过程中，它是二分类函数sigmoid在多分类上的推广，目的是<strong>将多分类的结果以概率的形式展现出来</strong>。</p>
<p>计算方法如下图所示：</p>
<p><img src="/images/NN_base/29.png" alt="29"></p>
<p><img src="/images/NN_base/30.png" alt="30"></p>
<p>SoftMax就是将网络输出的logits通过softmax函数，映射成为(0,1)的值，而这些值的累和为1（满足概率的性质），那么我们将它理解成概率，选取概率最大（也就是值对应最大的）节点，作为我们的预测目标类别。</p>
<p>在 PyTorch 中使用SoftMax函数的示例代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scores = torch.tensor([<span class="number">0.2</span>, <span class="number">0.02</span>, <span class="number">0.15</span>, <span class="number">0.15</span>, <span class="number">1.3</span>, <span class="number">0.5</span>, <span class="number">0.06</span>, <span class="number">1.1</span>, <span class="number">0.05</span>, <span class="number">3.75</span>])</span><br><span class="line"><span class="comment"># dim=0, 按行计算</span></span><br><span class="line">probabilities = torch.softmax(scores, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(probabilities)</span><br></pre></td></tr></table></figure>

<p>程序输出结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.0212</span>, <span class="number">0.0177</span>, <span class="number">0.0202</span>, <span class="number">0.0202</span>, <span class="number">0.0638</span>, <span class="number">0.0287</span>, <span class="number">0.0185</span>, <span class="number">0.0522</span>, <span class="number">0.0183</span>,</span><br><span class="line">        <span class="number">0.7392</span>])</span><br></pre></td></tr></table></figure>

<h4 id="如何选择激活函数"><a href="#如何选择激活函数" class="headerlink" title="如何选择激活函数"></a>如何选择激活函数</h4><blockquote>
<p>除了上述的激活函数，还存在很多其他的激活函数，如下图所示:</p>
<p><img src="/images/NN_base/31.png" alt="31"></p>
</blockquote>
<p>对于<strong>隐藏层</strong>:</p>
<ol>
<li>优先选择ReLU激活函数</li>
<li>如果ReLu效果不好，那么尝试其他激活，如Leaky ReLu等。</li>
<li>如果你使用了ReLU， 需要注意一下Dead ReLU问题，避免出现0梯度从而导致过多的神经元死亡。</li>
<li>少使用sigmoid激活函数，可以尝试使用tanh激活函数</li>
</ol>
<p>对于<strong>输出层</strong>:</p>
<ol>
<li>二分类问题选择sigmoid激活函数</li>
<li>多分类问题选择softmax激活函数</li>
<li>回归问题选择identity激活函数</li>
</ol>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>我们在构建网络之后，网络中的参数是需要初始化的。我们需要初始化的参数主要有<strong>权重</strong>和<strong>偏置</strong>，<strong>偏置一般初始化为0即可</strong>，而对权重的初始化则会更加重要。</p>
<p>参数初始化的作用：</p>
<ul>
<li><strong>防止梯度消失或爆炸</strong>：初始权重值过大或过小会导致梯度在反向传播中指数级增大或缩小。</li>
<li><strong>提高收敛速度</strong>：合理的初始化使得网络的激活值分布适中，有助于梯度高效更新。</li>
<li><strong>保持对称性破除</strong>：权重的初始化需要打破对称性，否则网络的学习能力会受到限制。</li>
</ul>
<h4 id="常见参数初始化方法"><a href="#常见参数初始化方法" class="headerlink" title="常见参数初始化方法"></a>常见参数初始化方法</h4><ul>
<li><p><strong>随机初始化</strong></p>
<ul>
<li><p>均匀分布初始化：权重参数初始化从区间均匀随机取值，默认区间为（0，1）。可以设置为在(-$$1\over\sqrt{d}$$,$$1\over\sqrt{d}​$$)均匀分布中生成当前神经元的权重，其中d为神经元的输入数量。</p>
</li>
<li><p>正态分布初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数W进行初始化</p>
</li>
<li><p><strong>优点</strong>：能有效打破对称性</p>
</li>
<li><p><strong>缺点</strong>：随机选择范围不当可能导致梯度问题</p>
</li>
<li><p><strong>适用场景</strong>：浅层网络或低复杂度模型。隐藏层1-3层，总层数不超过5层。</p>
</li>
</ul>
</li>
<li><p><strong>全0初始化</strong>：将神经网络中的所有权重参数初始化为0</p>
<ul>
<li><strong>优点</strong>：实现简单</li>
<li><strong>缺点</strong>：无法打破对称性，所有神经元更新方向相同，无法有效训练</li>
<li><strong>适用场景</strong>：几乎不使用，仅用于偏置项的初始化</li>
</ul>
</li>
<li><p><strong>全1初始化</strong>：将神经网络中的所有权重参数初始化为1</p>
<ul>
<li><strong>优点</strong>：实现简单</li>
<li><strong>缺点</strong><ul>
<li>无法打破对称性，所有神经元更新方向相同，无法有效训练</li>
<li>会导致激活值在网络中呈指数增长，容易出现梯度爆炸</li>
</ul>
</li>
<li><strong>适用场景</strong><ul>
<li>测试或调试：比如验证神经网络是否能正常前向传播和反向传播</li>
<li>特殊模型结构：某些稀疏网络或特定的自定义网络中可能需要手动设置部分参数为1</li>
<li>偏置初始化：偶尔可以将偏置初始化为小的正值（如 0.1），但很少用1作为偏置的初始值</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>固定值初始化</strong>：将神经网络中的所有权重参数初始化为某个固定值</p>
<ul>
<li><strong>优点</strong>：实现简单</li>
<li><strong>缺点</strong><ul>
<li>无法打破对称性，所有神经元更新方向相同，无法有效训练</li>
<li>初始权重过大或过小可能导致梯度爆炸或梯度消失</li>
</ul>
</li>
<li><strong>适用场景</strong><ul>
<li>测试或调试</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>kaiming初始化</strong>，也叫做<strong>HE初始化</strong>：专为ReLU和其变体设计，考虑到ReLU激活函数的特性，对输入维度进行缩放</p>
<ul>
<li>HE初始化分为正态分布的HE初始化、均匀分布的HE初始化<ul>
<li>正态分布的he初始化<ul>
<li>w权重值从均值为0, 标准差为std中随机采样，std &#x3D; <code>sqrt(2 / fan_in)</code></li>
<li>std值越大，w权重值离均值0分布相对较广，计算得到的内部状态值有较大的正值或负值</li>
</ul>
</li>
<li>均匀分布的he初始化<ul>
<li>它从[-limit，limit] 中的均匀分布中抽取样本, <code>limit</code> 是 <code>sqrt(6 / fan_in)</code></li>
</ul>
</li>
<li><code>fan_in</code> 输入神经元的个数，<strong>当前层</strong>接受的<strong>来自上一层</strong>的神经元的数量。简单来说，就是当前层接收多少个输入</li>
</ul>
</li>
<li><strong>优点</strong>：适合 ReLU，能保持梯度稳定</li>
<li><strong>缺点</strong>：对非 ReLU 激活函数效果一般</li>
<li><strong>适用场景</strong>：深度网络(10层及以上)，使用 ReLU、Leaky ReLU 激活函数</li>
</ul>
</li>
<li><p><strong>xavier初始化</strong>，也叫做<strong>Glorot初始化</strong>：根据网络输入和输出的维度自动选择权重范围，使输入和输出的方差相同</p>
<ul>
<li><p>xavier初始化分为正态分布的xavier初始化、均匀分布的xavier初始化</p>
<ul>
<li>正态化的Xavier初始化<ul>
<li>w权重值从均值为0, 标准差为std中随机采样，std &#x3D; <code>sqrt(2 / (fan_in + fan_out))</code></li>
<li>std值越小，w权重值离均值0分布相对集中，计算得到的内部状态值有较小的正值或负值</li>
</ul>
</li>
<li>均匀分布的Xavier初始化<ul>
<li>[-limit，limit] 中的均匀分布中抽取样本, limit 是 <code>sqrt(6 / (fan_in + fan_out))</code></li>
</ul>
</li>
<li>fan_in 是输入神经元个数，<strong>当前层</strong>接受的<strong>来自上一层</strong>的神经元的数量。简单来说，就是当前层接收多少个输入</li>
<li>fan_out 是输出神经元个数，<strong>当前层</strong>输出的神经元的数量，也就是当前层会传递给<strong>下一层</strong>的神经元的数量。简单来说，就是当前层会产生多少个输出。</li>
</ul>
</li>
<li><p><strong>优点</strong>：适用于Sigmoid、Tanh 等激活函数，解决梯度消失问题</p>
</li>
<li><p><strong>缺点</strong>：对 ReLU 等激活函数表现欠佳</p>
</li>
<li><p><strong>适用场景</strong>：深度网络(10层及以上)，使用 Sigmoid 或 Tanh 激活函数</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 均匀分布随机初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test01</span>():</span><br><span class="line"></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 从0-1均匀分布产生参数</span></span><br><span class="line">    nn.init.uniform_(linear.weight)</span><br><span class="line">    nn.init.uniform_(linear.bias)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 固定初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test02</span>():</span><br><span class="line"></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.constant_(linear.weight, <span class="number">5</span>)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 全0初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test03</span>():</span><br><span class="line"></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.zeros_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 全1初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test04</span>():</span><br><span class="line"></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.ones_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 正态分布随机初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test05</span>():</span><br><span class="line"></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.normal_(linear.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. kaiming 初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test06</span>():</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kaiming 正态分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.kaiming_normal_(linear.weight, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># kaiming 均匀分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.kaiming_uniform_(linear.weight, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. xavier 初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test07</span>():</span><br><span class="line"></span><br><span class="line">    <span class="comment"># xavier 正态分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.xavier_normal_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># xavier 均匀分布初始化</span></span><br><span class="line">    linear = nn.Linear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    nn.init.xavier_uniform_(linear.weight)</span><br><span class="line">    <span class="built_in">print</span>(linear.weight.data)</span><br></pre></td></tr></table></figure>

<h4 id="如何选择参数初始化"><a href="#如何选择参数初始化" class="headerlink" title="如何选择参数初始化"></a>如何选择参数初始化</h4><ul>
<li><p>激活函数的选择：根据激活函数的类型选择对应的初始化方法</p>
<ul>
<li><strong>Sigmoid&#x2F;Tanh</strong>：xavier 初始化</li>
<li><strong>ReLU&#x2F;Leaky ReLU</strong>：kaiming 初始化</li>
</ul>
</li>
<li><p>神经网络模型的深度</p>
<ul>
<li>浅层网络：随机初始化即可</li>
<li>深层网络：需要考虑方差平衡，如 xavier 或 kaiming 初始化</li>
</ul>
</li>
</ul>
<h3 id="神经网络搭建和参数计算"><a href="#神经网络搭建和参数计算" class="headerlink" title="神经网络搭建和参数计算"></a>神经网络搭建和参数计算</h3><h4 id="构建神经网络"><a href="#构建神经网络" class="headerlink" title="构建神经网络"></a>构建神经网络</h4><p>在pytorch中定义深度神经网络其实就是层堆叠的过程，继承自nn.Module，实现两个方法：</p>
<ul>
<li><code>__init__</code>方法中定义网络中的层结构，主要是全连接层，并进行初始化</li>
<li>forward方法，在调用神经网络模型对象的时候，底层会自动调用该函数。该函数中为初始化定义的layer传入数据，进行前向传播等。</li>
</ul>
<p>接下来我们来构建如下图所示的神经网络模型：</p>
<p><img src="/images/NN_base/image-20220313133601678.png" alt="image-20220313133601678"></p>
<p><strong>编码设计如下：</strong></p>
<ul>
<li>第1个隐藏层：权重初始化采用标准化的xavier初始化 激活函数使用sigmoid</li>
<li>第2个隐藏层：权重初始化采用标准化的He初始化 激活函数采用relu</li>
<li>out输出层线性层 假若多分类，采用softmax做数据归一化</li>
</ul>
<p><strong>构造神经网络模型代码:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary  <span class="comment"># 计算模型参数,查看模型结构, pip install torchsummary -i https://mirrors.aliyun.com/pypi/simple/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建神经网络模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化属性值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用父类的初始化属性值，确保nn.Module的初始化代码能够正确执行</span></span><br><span class="line">        <span class="built_in">super</span>(Model, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 创建第一个隐藏层模型, 3个输入特征,3个输出特征</span></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        nn.init.xavier_normal_(<span class="variable language_">self</span>.linear1.weight)</span><br><span class="line">        nn.init.zeros_(<span class="variable language_">self</span>.linear1.bias)</span><br><span class="line">        <span class="comment"># 创建第二个隐藏层模型, 3个输入特征(上一层的输出特征),2个输出特征</span></span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.linear2.weight, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        nn.init.zeros_(<span class="variable language_">self</span>.linear2.bias)</span><br><span class="line">        <span class="comment"># 创建输出层模型</span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 创建前向传播方法, 调用神经网络模型对象时自动执行forward()方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 数据经过第一个线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear1(x)</span><br><span class="line">        <span class="comment"># 使用sigmoid激活函数</span></span><br><span class="line">        x = torch.sigmoid(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据经过第二个线性层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.linear2(x)</span><br><span class="line">        <span class="comment"># 使用relu激活函数</span></span><br><span class="line">        x = torch.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据经过输出层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.out(x)</span><br><span class="line">        <span class="comment"># 使用softmax激活函数</span></span><br><span class="line">        <span class="comment"># dim=-1:每一维度行数据相加为1</span></span><br><span class="line">        x = torch.softmax(x, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p><strong>训练神经网络模型代码:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建构造模型函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="comment"># 实例化model对象</span></span><br><span class="line">    my_model = Model()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机产生数据</span></span><br><span class="line">    my_data = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_data--&gt;&quot;</span>, my_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_data shape&quot;</span>, my_data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据经过神经网络模型训练</span></span><br><span class="line">    output = my_model(my_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output--&gt;&quot;</span>, output)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output shape--&gt;&quot;</span>, output.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算模型参数</span></span><br><span class="line">    <span class="comment"># 计算每层每个神经元的w和b个数总和</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;======计算模型参数======&quot;</span>)</span><br><span class="line">    summary(my_model, input_size=(<span class="number">3</span>,), batch_size=<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 查看模型参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;======查看模型参数w和b======&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> name, parameter <span class="keyword">in</span> my_model.named_parameters():</span><br><span class="line">        <span class="built_in">print</span>(name, parameter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure>

<h4 id="观察数据形状变化"><a href="#观察数据形状变化" class="headerlink" title="观察数据形状变化"></a>观察数据形状变化</h4><ul>
<li><p>观察程序输入和输出的数据形状变化</p>
<ul>
<li>输入5行数据，输出也是5行数据</li>
<li>输入5行数据3个特征，经过第一个隐藏层是3个特征，经过第二个隐藏层是2个特征，经过输出层是2个特征</li>
<li>模型最终预测结果是：5行2列数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mydata.shape---&gt; torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br><span class="line">output.shape---&gt; torch.Size([<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">mydata---&gt;</span><br><span class="line">  tensor([[-<span class="number">0.3714</span>, -<span class="number">0.8578</span>, -<span class="number">1.6988</span>],</span><br><span class="line">        [ <span class="number">0.3149</span>,  <span class="number">0.0142</span>, -<span class="number">1.0432</span>],</span><br><span class="line">        [ <span class="number">0.5374</span>, -<span class="number">0.1479</span>, -<span class="number">2.0006</span>],</span><br><span class="line">        [ <span class="number">0.4327</span>, -<span class="number">0.3214</span>,  <span class="number">1.0928</span>],</span><br><span class="line">        [ <span class="number">2.2156</span>, -<span class="number">1.1640</span>,  <span class="number">1.0289</span>]])</span><br><span class="line">output---&gt;</span><br><span class="line"> tensor([[<span class="number">0.5095</span>, <span class="number">0.4905</span>],</span><br><span class="line">        [<span class="number">0.5218</span>, <span class="number">0.4782</span>],</span><br><span class="line">        [<span class="number">0.5419</span>, <span class="number">0.4581</span>],</span><br><span class="line">        [<span class="number">0.5163</span>, <span class="number">0.4837</span>],</span><br><span class="line">        [<span class="number">0.6030</span>, <span class="number">0.3970</span>]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="模型参数计算"><a href="#模型参数计算" class="headerlink" title="模型参数计算"></a>模型参数计算</h4><ul>
<li><p>模型参数的计算</p>
<ul>
<li><p>以第一个隐层为例：该隐层有3个神经元，每个神经元的参数为：4个（w1,w2,w3,b1），所以一共用3x4&#x3D;12个参数。 </p>
</li>
<li><p>输入数据和网络权重是两个不同的事儿！对于初学者理解这一点十分重要，要分得清。</p>
<p><img src="/images/NN_base/image-20220313141138281.png" alt="image-20220313141138281"></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">----------------------------------------------------------------</span><br><span class="line">        Layer (<span class="built_in">type</span>)               Output Shape         Param <span class="comment">#</span></span><br><span class="line">================================================================</span><br><span class="line">            Linear-<span class="number">1</span>                     [<span class="number">5</span>, <span class="number">3</span>]              <span class="number">12</span></span><br><span class="line">            Linear-<span class="number">2</span>                     [<span class="number">5</span>, <span class="number">2</span>]               <span class="number">8</span></span><br><span class="line">            Linear-<span class="number">3</span>                     [<span class="number">5</span>, <span class="number">2</span>]               <span class="number">6</span></span><br><span class="line">================================================================</span><br><span class="line">Total params: <span class="number">26</span></span><br><span class="line">Trainable params: <span class="number">26</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">Input size (MB): <span class="number">0.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">0.00</span></span><br><span class="line">Params size (MB): <span class="number">0.00</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">0.00</span></span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="查看模型参数"><a href="#查看模型参数" class="headerlink" title="查看模型参数"></a>查看模型参数</h4><ul>
<li><p>通常继承nn.Module，撰写自己的网络层。它强大的封装不需要我们定义可学习的参数（比如卷积核的权重和偏置参数）。</p>
</li>
<li><p>如何才能查看封装好的，可学习的网络参数哪？</p>
<ul>
<li>模块实例名.name_parameters（）,会分别返回name和parameter</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化model对象</span></span><br><span class="line">mymodel = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看网络参数</span></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> mymodel.named_parameters():</span><br><span class="line">    <span class="comment"># print(&#x27;name---&gt;&#x27;, name)</span></span><br><span class="line">    <span class="comment"># print(&#x27;parameter---&gt;&#x27;, parameter)</span></span><br><span class="line">    <span class="built_in">print</span>(name, parameter)</span><br></pre></td></tr></table></figure>

<p><strong>结果显示</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">linear1.weight Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.1715</span>, -<span class="number">0.3711</span>,  <span class="number">0.1692</span>],</span><br><span class="line">        [-<span class="number">0.2497</span>, -<span class="number">0.6156</span>, -<span class="number">0.4235</span>],</span><br><span class="line">        [-<span class="number">0.7090</span>, -<span class="number">0.0380</span>,  <span class="number">0.4790</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">linear1.bias Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.2320</span>,  <span class="number">0.3431</span>,  <span class="number">0.2771</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">linear2.weight Parameter containing:</span><br><span class="line">tensor([[-<span class="number">0.5044</span>, -<span class="number">0.7435</span>, -<span class="number">0.6736</span>],</span><br><span class="line">        [ <span class="number">0.6908</span>, -<span class="number">0.1466</span>, -<span class="number">0.0019</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">linear2.bias Parameter containing:</span><br><span class="line">tensor([<span class="number">0.2340</span>, <span class="number">0.4730</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">out.weight Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.5185</span>,  <span class="number">0.4019</span>],</span><br><span class="line">        [-<span class="number">0.4313</span>, -<span class="number">0.3438</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">out.bias Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.4521</span>, -<span class="number">0.6339</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="损失函数概念"><a href="#损失函数概念" class="headerlink" title="损失函数概念"></a>损失函数概念</h3><p>在深度学习中, 损失函数是用来&#x3D;&#x3D;衡量模型参数质量的函数&#x3D;&#x3D;, 衡量的方式<strong>是比较网络输出（预测值）和真实输出（真实值）的差异。</strong></p>
<p>模型通过最小化损失函数的值来调整参数，使其输出更接近真实值。</p>
<p><img src="/images/NN_base/03-1.png"></p>
<p>损失函数在不同的文献中名称是不一样的，主要有以下几种<strong>命名方式</strong>：</p>
<p><img src="/images/NN_base/03-2.png"></p>
<p><strong>损失函数作用：</strong></p>
<ul>
<li><strong>评估性能</strong>：反映模型预测结果与目标值的匹配程度。</li>
<li><strong>指导优化</strong>：通过梯度下降等算法最小化损失函数，优化模型参数。</li>
</ul>
<h3 id="分类任务损失函数"><a href="#分类任务损失函数" class="headerlink" title="分类任务损失函数"></a>分类任务损失函数</h3><blockquote>
<p>在深度学习的<strong>分类任务</strong>中使用最多的是<strong>交叉熵损失函数</strong>，所以在这里我们着重介绍这种损失函数。</p>
</blockquote>
<h4 id="多分类任务损失函数"><a href="#多分类任务损失函数" class="headerlink" title="多分类任务损失函数"></a>多分类任务损失函数</h4><p>在多分类任务通常使用softmax将logits转换为概率的形式，所以多分类的交叉熵损失也叫做<strong>softmax损失</strong>，它的计算方法是：</p>
<p><img src="/images/NN_base/03-3.png"></p>
<p>其中:</p>
<ul>
<li>$$y_i$$是样本x属于某一个类别的真实概率</li>
<li>而f(x)是样本属于某一类别的预测分数</li>
<li>S是softmax激活函数，将属于某一类别的预测分数转换成概率</li>
<li>L用来衡量真实值y和预测值f(x)之间差异性的损失结果</li>
</ul>
<p>例子：</p>
<p><img src="/images/NN_base/03-4.png" alt=" "></p>
<p>上图中的交叉熵损失为：</p>
<p><img src="/images/NN_base/03-5.png" alt=" "></p>
<p>从概率角度理解，我们的目的是最小化正确类别所对应的预测概率的对数的负值(损失值最小)，如下图所示：</p>
<p><img src="/images/NN_base/03-6.png" alt=" "></p>
<p>在PyTorch中使用<code>nn.CrossEntropyLoss()</code>实现，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类损失函数：交叉熵损失使用nn.CrossEntropyLoss()实现。nn.CrossEntropyLoss()=softmax+损失计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test01</span>():</span><br><span class="line">	<span class="comment"># 设置真实值: 可以是热编码后的结果也可以不进行热编码</span></span><br><span class="line">	<span class="comment"># y_true = torch.tensor([[0, 1, 0], [0, 0, 1]], dtype=torch.float32)</span></span><br><span class="line">	<span class="comment"># 注意：类型必须是64位整型数据</span></span><br><span class="line">	y_true = torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.int64)</span><br><span class="line">	y_pred = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.2</span>], [<span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.1</span>]], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">	<span class="comment"># 实例化交叉熵损失，默认求平均损失</span></span><br><span class="line">	<span class="comment"># reduction=&#x27;sum&#x27;：总损失</span></span><br><span class="line">	loss = nn.CrossEntropyLoss()</span><br><span class="line">	<span class="comment"># 计算损失结果</span></span><br><span class="line">	my_loss = loss(y_pred, y_true).detach().numpy()</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;loss:&#x27;</span>, my_loss)</span><br></pre></td></tr></table></figure>

<h4 id="二分类任务损失函数"><a href="#二分类任务损失函数" class="headerlink" title="二分类任务损失函数"></a>二分类任务损失函数</h4><p>在处理二分类任务时，我们不再使用softmax激活函数，而是使用sigmoid激活函数，那损失函数也相应的进行调整，使用二分类的交叉熵损失函数：</p>
<p><img src="/images/NN_base/03-7.png" alt=" "></p>
<p>其中:</p>
<ul>
<li><p>y是样本x属于某一个类别的真实概率</p>
</li>
<li><p>而$$\hat{y}$$是样本属于某一类别的预测概率</p>
</li>
<li><p>L用来衡量真实值y与预测值$$\hat{y}$$之间差异性的损失结果。</p>
<p><img src="/images/NN_base/1735117379356.png" alt=" "></p>
</li>
</ul>
<p>在PyTorch中实现时使用<code>nn.BCELoss() </code>实现，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test02</span>():</span><br><span class="line">    <span class="comment"># 1 设置真实值和预测值</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 预测值是sigmoid输出的结果</span></span><br><span class="line">    y_pred = torch.tensor([<span class="number">0.6901</span>, <span class="number">0.5459</span>, <span class="number">0.2469</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 2 实例化二分类交叉熵损失</span></span><br><span class="line">    loss = nn.BCELoss()</span><br><span class="line">    <span class="comment"># 3 计算损失</span></span><br><span class="line">    my_loss = loss(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss：&#x27;</span>, my_loss)</span><br></pre></td></tr></table></figure>

<h3 id="回归任务损失函数"><a href="#回归任务损失函数" class="headerlink" title="回归任务损失函数"></a>回归任务损失函数</h3><h4 id="MAE损失函数"><a href="#MAE损失函数" class="headerlink" title="MAE损失函数"></a>MAE损失函数</h4><p>**mean absolute loss(MAE)**也被称为L1 Loss，是以绝对误差作为距离</p>
<p>损失函数公式:</p>
<p><img src="/images/NN_base/03-8.png" alt=" "></p>
<p>曲线如下图所示：</p>
<p><img src="/images/NN_base/03-9.png" alt=" "></p>
<p>特点是：</p>
<ul>
<li>由于L1 loss具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。(0点不可导, 产生稀疏矩阵)</li>
<li>L1 loss的最大问题是梯度在零点不平滑，导致会跳过极小值</li>
<li>适用于回归问题中存在异常值或噪声数据时，可以减少对离群点的敏感性</li>
</ul>
<p>在PyTorch中使用<code>nn.L1Loss()</code>实现，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算inputs与target之差的绝对值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test03</span>():</span><br><span class="line">    <span class="comment"># 1 设置真实值和预测值</span></span><br><span class="line">    y_pred = torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.9</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y_true = torch.tensor([<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>], dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 2 实例MAE损失对象</span></span><br><span class="line">    loss = nn.L1Loss()</span><br><span class="line">    <span class="comment"># 3 计算损失</span></span><br><span class="line">    my_loss = loss(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss:&#x27;</span>, my_loss)</span><br></pre></td></tr></table></figure>

<h4 id="MSE损失函数"><a href="#MSE损失函数" class="headerlink" title="MSE损失函数"></a>MSE损失函数</h4><p>**Mean Squared Loss&#x2F; Quadratic Loss(MSE loss)**也被称为L2 loss，或欧氏距离，它以误差的平方和的均值作为距离</p>
<p>损失函数公式:</p>
<p><img src="/images/NN_base/03-10.png" alt="image-20200730165213969"></p>
<p>曲线如下图所示：</p>
<p><img src="/images/NN_base/03-11.png"></p>
<p>特点是：</p>
<ul>
<li><p>L2 loss也常常作为正则项，对于离群点（outliers）敏感，因为平方项会放大大误差</p>
</li>
<li><p>当预测值与目标值相差很大时, 梯度容易爆炸</p>
<ul>
<li>梯度爆炸:网络层之间的梯度（值大于1.0）重复相乘导致的指数级增长会产生梯度爆炸</li>
</ul>
</li>
<li><p>适用于大多数标准回归问题，如房价预测、温度预测等</p>
</li>
</ul>
<p>在PyTorch中通过<code>nn.MSELoss()</code>实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test04</span>():</span><br><span class="line">    <span class="comment"># 1 设置真实值和预测值</span></span><br><span class="line">    y_pred = torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.9</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y_true = torch.tensor([<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>], dtype=torch.float32)</span><br><span class="line">    <span class="comment"># 2 实例MSE损失对象</span></span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    <span class="comment"># 3 计算损失</span></span><br><span class="line">    my_loss = loss(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;myloss:&#x27;</span>, my_loss)</span><br></pre></td></tr></table></figure>

<h4 id="Smooth-L1损失函数"><a href="#Smooth-L1损失函数" class="headerlink" title="Smooth L1损失函数"></a>Smooth L1损失函数</h4><blockquote>
<p>smooth L1说的是光滑之后的L1，是一种结合了均方误差（MSE）和平均绝对误差（MAE）优点的损失函数。它在误差较小时表现得像 MSE，在误差较大时则更像 MAE。</p>
</blockquote>
<p>Smooth L1损失函数如下式所示：</p>
<p><img src="/images/NN_base/03-12.png" alt="image-20200730170635889"></p>
<p>其中：<code>𝑥=f(x)−y</code> 为真实值和预测值的差值。</p>
<p><img src="/images/NN_base/03-13.png"></p>
<p>从上图中可以看出，该函数实际上就是一个分段函数</p>
<ul>
<li>在[-1,1]之间实际上就是L2损失，这样解决了L1的不光滑问题</li>
<li>在[-1,1]区间外，实际上就是L1损失，这样就解决了离群点梯度爆炸的问题</li>
</ul>
<p>特点是：</p>
<ul>
<li><p><strong>对离群点更加鲁棒</strong>：当误差较大时，损失函数会线性增加（而不是像MSE那样平方增加），因此它对离群点的惩罚更小，避免了MSE对离群点过度敏感的问题</p>
</li>
<li><p><strong>计算梯度时更加平滑</strong>：与MAE相比，Smooth L1在小误差时表现得像MSE，避免了在训练过程中因使用绝对误差而导致的梯度不连续问题</p>
</li>
</ul>
<p>在PyTorch中使用<code>nn.SmoothL1Loss()</code>计算该损失，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test05</span>():</span><br><span class="line">    <span class="comment"># 1 设置真实值和预测值</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line">    y_pred = torch.tensor([<span class="number">0.6</span>, <span class="number">0.4</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 2 实例smmothL1损失对象</span></span><br><span class="line">    loss = nn.SmoothL1Loss()</span><br><span class="line">    <span class="comment"># 3 计算损失</span></span><br><span class="line">    my_loss = loss(y_pred, y_true).detach().numpy()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss:&#x27;</span>, my_loss)</span><br></pre></td></tr></table></figure>

<h2 id="神经网络优化方法"><a href="#神经网络优化方法" class="headerlink" title="神经网络优化方法"></a>神经网络优化方法</h2><p>多层神经网络的学习能力比单层网络强得多。想要训练多层网络，需要更强大的学习算法。误差反向传播算法（Back Propagation）是其中最杰出的代表，它是目前最成功的神经网络学习算法。现实任务使用神经网络时，大多是在使用 BP 算法进行训练，值得指出的是 BP 算法不仅可用于多层前馈神经网络，还可以用于其他类型的神经网络。通常说 BP 网络时，一般是指用 BP 算法训练的多层前馈神经网络。</p>
<p>这就需要了解两个概念：</p>
<ol>
<li><strong>正向传播</strong>：指的是数据通过网络从输入层到输出层的传递过程。这个过程的目的是计算网络的输出值（预测值），从而与目标值（真实值）比较以计算误差。</li>
<li><strong>反向传播</strong>：指的是计算损失函数相对于网络中各参数（权重和偏置）的梯度，指导优化器更新参数，从而使神经网络的预测更接近目标值。</li>
</ol>
<h3 id="梯度下降算法回顾"><a href="#梯度下降算法回顾" class="headerlink" title="梯度下降算法回顾"></a>梯度下降算法回顾</h3><p><strong>梯度下降法</strong>简单来说就是一种<strong>寻找使损失函数最小化的方法</strong>。</p>
<p>从数学角度来看，<strong>梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向</strong>，所以有：</p>
<p><img src="/images/NN_base/12.png"></p>
<p>其中，η是学习率，如果学习率<strong>太小</strong>，那么每次训练之后得到的效果都太小，<strong>增大训练的时间成本</strong>。如果，学习率<strong>太大</strong>，那就有可能<strong>直接跳过最优解</strong>，进入无限的训练中。解决的方法就是，学习率也需要随着训练的进行而变化。</p>
<p><img src="/images/NN_base/04-05.png"></p>
<p>在上图中我们展示了一维和多维的损失函数，损失函数呈碗状。在训练过程中损失函数对权重的偏导数就是损失函数在该位置点的梯度。我们可以看到，沿着负梯度方向移动，就可以到达损失函数底部，从而使损失函数最小化。这种利用损失函数的梯度迭代地寻找最小值的过程就是梯度下降的过程。</p>
<p><strong>在进行模型训练时，有三个基础的概念：</strong></p>
<ol>
<li><strong>Epoch</strong>: 使用全部数据对模型进行以此完整训练，训练次数</li>
<li><strong>Batch</strong>: 使用训练集中的小部分样本对模型权重进行以此反向传播的参数更新，每次训练每批次样本数量</li>
<li><strong>Iteration</strong>: 使用一个 Batch 数据对模型进行一次参数更新的过程，每次训练批次数</li>
</ol>
<p>假设数据集有 50000 个训练样本，现在选择 Batch Size &#x3D; 256 对模型进行训练。<br>每个 Epoch 要训练的图片数量：50000<br>训练集具有的 Batch 个数：50000&#x2F;256+1&#x3D;196<br>每个 Epoch 具有的 Iteration 个数：196<br>10个 Epoch 具有的 Iteration 个数：1960</p>
<p>在深度学习中，<strong>梯度下降的几种方式的根本区别就在于Batch Size不同</strong>,如下表所示：</p>
<p><img src="/images/NN_base/13.png"></p>
<p>&#x3D;&#x3D;注：上表中 Mini-Batch 的 Batch 个数为 N &#x2F; B + 1 是针对未整除的情况。整除则是 N &#x2F; B。&#x3D;&#x3D;</p>
<h3 id="反向传播（BP算法）"><a href="#反向传播（BP算法）" class="headerlink" title="反向传播（BP算法）"></a>反向传播（BP算法）</h3><blockquote>
<p>利用<strong>反向传播算法</strong>对神经网络进行训练。该方法与<strong>梯度下降算法</strong>相结合，对网络中所有权重<strong>计算损失函数的梯度</strong>，并利用梯度值来<strong>更新权值</strong>以最小化损失函数。</p>
</blockquote>
<h4 id="反向传播概念"><a href="#反向传播概念" class="headerlink" title="反向传播概念"></a>反向传播概念</h4><p><strong>前向传播</strong>：指的是数据输入到神经网络中，逐层向前传输，一直运算到输出层为止。</p>
<p><strong>反向传播（Back Propagation）</strong>：利用损失函数ERROR值，从后往前，结合梯度下降算法，依次求各个参数的偏导，并进行参数更新。</p>
<p><img src="/images/NN_base/14.png"></p>
<p>在网络的训练过程中经过前向传播后得到的最终结果跟训练样本的真实值总是存在一定误差，这个误差便是损失函数 ERROR。想要减小这个误差，<strong>就用损失函数 ERROR，从后往前，依次求各个参数的偏导，这就是反向传播（Back Propagation）</strong>。</p>
<h4 id="反向传播详解"><a href="#反向传播详解" class="headerlink" title="反向传播详解"></a>反向传播详解</h4><p><strong>反向传播算法利用链式法则对神经网络中的各个节点的权重进行更新</strong>。</p>
<p>【举个栗子🌰：】</p>
<p>如下图是一个简单的神经网络用来举例：<strong>激活函数为sigmoid</strong></p>
<p><img src="/images/NN_base/image-20210129103846076.png" alt=" "></p>
<p><strong>前向传播运算</strong>：</p>
<p><img src="/images/NN_base/image-20210129104612230.png" alt=" "></p>
<p>接下来是<strong>反向传播</strong>（求网络误差对各个权重参数的梯度）：</p>
<p>我们先来求最简单的，求误差E对w5的导数。首先明确这是一个<strong>链式法则</strong>的求导过程，要求误差E对w5的导数，需要先求误差E对$$out_{o1}$$的导数，再求$$out_{o1}$$对$$net_{o1}$$的导数，最后再求$$net_{o1}$$对$$w_5$$的导数，经过这个<strong>链式法则</strong>，我们就可以求出误差E对$$w_5$$的导数（偏导），如下图所示：</p>
<p><img src="/images/NN_base/image-20210129104646474.png" alt=" "></p>
<p>导数（梯度）已经计算出来了，下面就是<strong>反向传播与参数更新过程</strong>：</p>
<p><img src="/images/NN_base/image-20210129104718140.png" alt=" "></p>
<p>如果要想求<strong>误差E对w1的导数</strong>，误差E对w1的求导路径不止一条，这会稍微复杂一点，但换汤不换药，计算过程如下所示：</p>
<p><img src="/images/NN_base/image-20210129104756053.png" alt=" "></p>
<p>至此，<strong>反向传播算法</strong>的过程就讲完了啦！</p>
<h3 id="梯度下降优化方法"><a href="#梯度下降优化方法" class="headerlink" title="梯度下降优化方法"></a>梯度下降优化方法</h3><p>梯度下降优化算法中，可能会碰到以下情况：</p>
<ul>
<li>碰到平缓区域，梯度值较小，参数优化变慢</li>
<li>碰到 “鞍点” ，梯度为0，参数无法优化</li>
<li>碰到局部最小值，参数不是最优</li>
</ul>
<p>对于这些问题, 出现了一些对梯度下降算法的优化方法，例如：<strong>Momentum</strong>、<strong>AdaGrad</strong>、<strong>RMSprop</strong>、<strong>Adam</strong> 等</p>
<p><img src="/images/NN_base/50.png"></p>
<h4 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h4><p>我们最常见的算数平均指的是将所有数加起来除以数的个数，每个数的权重是相同的。指数加权平均指的是给每个数赋予不同的权重求得平均数。移动平均数，指的是计算最近邻的 N 个数来获得平均数。</p>
<p><strong>指数移动加权平均</strong>则是参考各数值，并且各数值的权重都不同，距离越远的数字对平均数计算的贡献就越小（权重较小），距离越近则对平均数的计算贡献就越大（权重越大）。</p>
<p>比如：明天气温怎么样，和昨天气温有很大关系，而和一个月前的气温关系就小一些。</p>
<p>计算公式可以用下面的式子来表示：</p>
<p><img src="/images/NN_base/37.png"></p>
<ul>
<li>$S_t$ 表示指数加权平均值;</li>
<li>$Y_t​$ 表示t时刻的值;</li>
<li>β 调节权重系数，该值越大平均数越平缓。</li>
</ul>
<p><strong>第100天的指数加权平均值为:</strong></p>
<p><img src="/images/NN_base/1735177998337.png" alt="1735177998337"></p>
<p><strong>下面通过代码来看结果，随机产生 30 天的气温数据：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ELEMENT_NUMBER = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 实际平均温度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test01</span>():</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 固定随机数种子</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 产生30天的随机温度</span></span><br><span class="line">    temperature = torch.randn(size=[ELEMENT_NUMBER,]) * <span class="number">10</span></span><br><span class="line">    <span class="built_in">print</span>(temperature)</span><br><span class="line">    <span class="comment"># 绘制平均温度</span></span><br><span class="line">    days = torch.arange(<span class="number">1</span>, ELEMENT_NUMBER + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(days, temperature, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.scatter(days, temperature)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 指数加权平均温度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test02</span>(<span class="params">beta=<span class="number">0.9</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 固定随机数种子</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 产生30天的随机温度</span></span><br><span class="line">    temperature = torch.randn(size=[ELEMENT_NUMBER,]) * <span class="number">10</span></span><br><span class="line">    <span class="built_in">print</span>(temperature)</span><br><span class="line"></span><br><span class="line">    exp_weight_avg = []</span><br><span class="line">    <span class="comment"># idx从1开始</span></span><br><span class="line">    <span class="keyword">for</span> idx, temp <span class="keyword">in</span> <span class="built_in">enumerate</span>(temperature, <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 第一个元素的 EWA 值等于自身</span></span><br><span class="line">        <span class="keyword">if</span> idx == <span class="number">1</span>:</span><br><span class="line">            exp_weight_avg.append(temp)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># 第二个元素的 EWA 值等于上一个 EWA 乘以 β + 当前气温乘以 (1-β)</span></span><br><span class="line">        <span class="comment"># idx-2：2-2=0，exp_weight_avg列表中第一个值的下标值</span></span><br><span class="line">        new_temp = exp_weight_avg[idx - <span class="number">2</span>] * beta + (<span class="number">1</span> - beta) * temp</span><br><span class="line">        exp_weight_avg.append(new_temp)</span><br><span class="line"></span><br><span class="line">    days = torch.arange(<span class="number">1</span>, ELEMENT_NUMBER + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(days, exp_weight_avg, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.scatter(days, temperature)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test01()</span><br><span class="line">    test02(<span class="number">0.5</span>)</span><br><span class="line">    test02(<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/images/NN_base/1734144867552.png" alt="1734144867552"></p>
<p><strong>从程序运行结果可以看到：</strong></p>
<ul>
<li>指数加权平均绘制出的气温变化曲线更加&#x3D;&#x3D;平缓&#x3D;&#x3D;</li>
<li>β 的值越大，则绘制出的折线&#x3D;&#x3D;越加平缓，波动越小&#x3D;&#x3D;(1-β越小,t时刻的$S_t$越不依赖$Y_t$的值)</li>
<li>β 值一般默认都是 0.9</li>
</ul>
<h4 id="动量算法Momentum"><a href="#动量算法Momentum" class="headerlink" title="动量算法Momentum"></a>动量算法Momentum</h4><p>当梯度下降碰到 “峡谷” 、”平缓”、”鞍点” 区域时, 参数更新速度变慢。 Momentum 通过<strong>指数加权平均法</strong>，累计历史梯度值，进行参数更新，越近的梯度值对当前参数更新的重要性越大。</p>
<p><strong>梯度计算公式</strong>：</p>
<p>​				$$s_t&#x3D;βs_{t−1}+(1−β)g_t$$</p>
<p><strong>参数更新公式</strong>：</p>
<p>​				$$w_t&#x3D;w_{t−1}−ηs_t$$</p>
<p>$s_t$是当前时刻指数加权平均梯度值</p>
<p>$s_{t-1}$是历史指数加权平均梯度值</p>
<p>$g_t$是当前时刻的梯度值</p>
<p>β 是调节权重系数，通常取 0.9 或 0.99</p>
<p>η是学习率</p>
<p>$w_t$是当前时刻模型权重参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">咱们举个例子，假设：权重 β 为 <span class="number">0.9</span>，例如：</span><br><span class="line">第一次梯度值：s1 = g1 = w1 </span><br><span class="line">第二次梯度值：s2 = <span class="number">0.9</span>*s1 + g2*<span class="number">0.1</span> </span><br><span class="line">第三次梯度值：s3 = <span class="number">0.9</span>*s2 + g3*<span class="number">0.1</span> </span><br><span class="line">第四次梯度值：s4 = <span class="number">0.9</span>*s3 + g4*<span class="number">0.1</span> </span><br><span class="line"><span class="number">1.</span> w 表示初始梯度</span><br><span class="line"><span class="number">2.</span> g 表示当前轮数计算出的梯度值</span><br><span class="line"><span class="number">3.</span> s 表示历史梯度移动加权平均值</span><br><span class="line"></span><br><span class="line">梯度下降公式中梯度的计算，就不再是当前时刻t的梯度值，而是历史梯度值的指数移动加权平均值。</span><br><span class="line">公式修改为：</span><br><span class="line">Wt = Wt-<span class="number">1</span> - η*St</span><br><span class="line">Wt：当前时刻模型权重参数</span><br><span class="line">St：当前时刻指数加权平均梯度值</span><br><span class="line">η：学习率</span><br></pre></td></tr></table></figure>

<p>Monmentum 优化方法是如何一定程度上克服 “平缓”、”鞍点”、”峡谷” 的问题呢？</p>
<p><img src="/images/NN_base/42.png"></p>
<ul>
<li>当处于鞍点位置时，由于当前的梯度为 0，参数无法更新。但是 Momentum 动量梯度下降算法已经在先前积累了一些梯度值，很有可能使得跨过鞍点。</li>
<li>由于 mini-batch 普通的梯度下降算法，每次选取少数的样本梯度确定前进方向，可能会出现震荡，使得训练时间变长。Momentum 使用移动加权平均，平滑了梯度的变化，使得前进方向更加平缓，有利于加快训练过程。一定程度上有利于降低 “峡谷” 问题的影响。</li>
</ul>
<p>在pytorch中动量梯度优化法编程实践如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test01</span>():</span><br><span class="line">    <span class="comment"># 1 初始化权重参数</span></span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 2 实例化优化方法：SGD 指定参数beta=0.9</span></span><br><span class="line">    optimizer = torch.optim.SGD([w], lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 3 第1次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第1次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br><span class="line">    <span class="comment"># 4 第2次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    <span class="comment"># 使用更新后的参数机选输出结果</span></span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第2次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br></pre></td></tr></table></figure>

<p><strong>结果显示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>次: 梯度w.grad: <span class="number">1.000000</span>, 更新后的权重:<span class="number">0.990000</span></span><br><span class="line">第<span class="number">2</span>次: 梯度w.grad: <span class="number">0.990000</span>, 更新后的权重:<span class="number">0.971100</span></span><br></pre></td></tr></table></figure>

<h4 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h4><p>AdaGrad 通过对不同的参数分量使用不同的学习率，<strong>AdaGrad 的学习率总体会逐渐减小</strong>，这是因为 AdaGrad 认为：在起初时，我们距离最优目标仍较远，可以使用较大的学习率，加快训练速度，随着迭代次数的增加，学习率逐渐下降。</p>
<p>其计算步骤如下：</p>
<ol>
<li><p>初始化学习率 η、初始化参数w、小常数 σ &#x3D; 1e-10</p>
</li>
<li><p>初始化梯度累计变量 s &#x3D; 0</p>
</li>
<li><p>从训练集中采样 m 个样本的小批量，计算梯度$g_t​$</p>
</li>
<li><p><strong>累积平方梯度</strong>: $s_t​$ &#x3D; $s_{t-1}​$ + $g_t​$ ⊙ $g_t​$，⊙ 表示各个分量相乘</p>
</li>
<li><p>学习率 η 的计算公式如下：</p>
<p>   ​            η &#x3D; $$η\over\sqrt{s_t}+σ$$</p>
</li>
<li><p>权重参数更新公式如下：</p>
<p>   ​            $w_t$ &#x3D; $$w_{t-1}$$ - $$η\over\sqrt{s_t}+σ$$ * $g_t$</p>
</li>
<li><p>重复 3-7 步骤</p>
</li>
</ol>
<p><strong>AdaGrad 缺点是可能会使得学习率过早、过量的降低，导致模型训练后期学习率太小，较难找到最优解。</strong></p>
<p>在PyTorch中AdaGrad优化法编程实践如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test02</span>():</span><br><span class="line">    <span class="comment"># 1 初始化权重参数</span></span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 2 实例化优化方法：adagrad优化方法</span></span><br><span class="line">    optimizer = torch.optim.Adagrad([w], lr=<span class="number">0.01</span>)</span><br><span class="line">    <span class="comment"># 3 第1次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第1次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br><span class="line">    <span class="comment"># 4 第2次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    <span class="comment"># 使用更新后的参数机选输出结果</span></span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第2次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br></pre></td></tr></table></figure>

<p><strong>结果显示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>次: 梯度w.grad: <span class="number">1.000000</span>, 更新后的权重:<span class="number">0.990000</span></span><br><span class="line">第<span class="number">2</span>次: 梯度w.grad: <span class="number">0.990000</span>, 更新后的权重:<span class="number">0.982965</span></span><br></pre></td></tr></table></figure>

<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p><strong>RMSProp 优化算法是对 AdaGrad 的优化</strong>。最主要的不同是，其使用<strong>指数加权平均梯度</strong>替换历史梯度的平方和。</p>
<p>其计算过程如下：</p>
<ol>
<li><p>初始化学习率 η、初始化权重参数w、小常数 σ &#x3D; 1e-10</p>
</li>
<li><p>初始化梯度累计变量 s &#x3D; 0</p>
</li>
<li><p>从训练集中采样 m 个样本的小批量，计算梯度 $g_t$</p>
</li>
<li><p>使用指数加权平均累计历史梯度，⊙ 表示各个分量相乘，公式如下：</p>
<p>   ​            $s_t$ &#x3D; β$s_{t-1}$ + (1-β)$g_t$⊙$g_t$</p>
</li>
<li><p>学习率 η 的计算公式如下：</p>
<p>   ​            η &#x3D; $η\over\sqrt{s_t}+σ​$</p>
</li>
<li><p>权重参数更新公式如下：</p>
<p>   ​            $w_t$ &#x3D; $w_{t-1}$ - $η\over\sqrt{s_t}+σ$ * $g_t$</p>
</li>
<li><p>重复 3-7 步骤</p>
</li>
</ol>
<p>RMSProp 与 AdaGrad 最大的区别是对梯度的累积方式不同，对于每个梯度分量仍然使用不同的学习率。</p>
<p><strong>RMSProp 通过引入衰减系数β，控制历史梯度对历史梯度信息获取的多少</strong>. 被证明在神经网络非凸条件下的优化更好，学习率衰减更加合理一些。</p>
<p>需要注意的是：AdaGrad 和 RMSProp 都是对于不同的参数分量使用不同的学习率，如果某个参数分量的梯度值较大，则对应的学习率就会较小，如果某个参数分量的梯度较小，则对应的学习率就会较大一些。</p>
<p>在PyTorch中RMSprop梯度优化法，编程实践如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test03</span>():</span><br><span class="line">    <span class="comment"># 1 初始化权重参数</span></span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>, dtype=torch.float32)</span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 2 实例化优化方法：RMSprop算法，其中alpha对应beta</span></span><br><span class="line">    optimizer = torch.optim.RMSprop([w], lr=<span class="number">0.01</span>, alpha=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 3 第1次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第1次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br><span class="line">    <span class="comment"># 4 第2次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    <span class="comment"># 使用更新后的参数机选输出结果</span></span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第2次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br></pre></td></tr></table></figure>

<p><strong>结果显示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>次: 梯度w.grad: <span class="number">1.000000</span>, 更新后的权重:<span class="number">0.968377</span></span><br><span class="line">第<span class="number">2</span>次: 梯度w.grad: <span class="number">0.968377</span>, 更新后的权重:<span class="number">0.945788</span></span><br></pre></td></tr></table></figure>

<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><ul>
<li><p>Momentum 使用指数加权平均计算当前的梯度值</p>
</li>
<li><p>AdaGrad、RMSProp 使用自适应的学习率</p>
</li>
<li><p>Adam优化算法（Adaptive Moment Estimation，自适应矩估计）将 Momentum 和 RMSProp 算法结合在一起</p>
<ul>
<li>修正梯度: 使⽤梯度的指数加权平均 </li>
<li>修正学习率: 使⽤梯度平⽅的指数加权平均</li>
</ul>
</li>
<li><p><strong>原理</strong>：Adam 是结合了 <strong>Momentum</strong> 和 <strong>RMSProp</strong> 优化算法的优点的自适应学习率算法。它计算了梯度的一阶矩（平均值）和二阶矩（梯度的方差）的自适应估计，从而动态调整学习率。</p>
</li>
<li><p><strong>梯度计算公式</strong>：</p>
<p>​        $$m_t&#x3D;β_1m_{t−1}+(1−β_1)g_t​$$</p>
<p>​        $$s_t&#x3D;β_2s_{t−1}+(1−β_2)gt^2​$$</p>
<p>​        $$\hat{m_t}​$$ &#x3D; $$m_t\over1−β_1^t​$$, $$\hat{s_t}​$$&#x3D;$$s_t\over1−β_2^t​$$</p>
</li>
<li><p><strong>权重参数更新公式</strong>:</p>
<p>   ​        $$w_t$$ &#x3D; $$w_{t−1}$$ − $$η\over\sqrt{\hat{s_t}}+ϵ$$$\hat{m_t}$</p>
</li>
</ul>
<p>其中，$$m_t$$ 是梯度的一阶矩估计，$$s_t$$ 是梯度的二阶矩估计，$$ \hat{m_t}$$和 $$\hat{s_t}$$ 是偏差校正后的估计。</p>
<p>在PyTroch中，Adam梯度优化法编程实践如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test04</span>():</span><br><span class="line">    <span class="comment"># 1 初始化权重参数</span></span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 2 实例化优化方法：Adam算法，其中betas是指数加权的系数</span></span><br><span class="line">    optimizer = torch.optim.Adam([w], lr=<span class="number">0.01</span>, betas=[<span class="number">0.9</span>, <span class="number">0.99</span>])</span><br><span class="line">    <span class="comment"># 3 第1次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第1次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br><span class="line">    <span class="comment"># 4 第2次更新 计算梯度，并对参数进行更新</span></span><br><span class="line">    <span class="comment"># 使用更新后的参数机选输出结果</span></span><br><span class="line">    loss = ((w ** <span class="number">2</span>) / <span class="number">2.0</span>).<span class="built_in">sum</span>()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;第2次: 梯度w.grad: %f, 更新后的权重:%f&#x27;</span> % (w.grad.numpy(), w.detach().numpy()))</span><br></pre></td></tr></table></figure>

<p><strong>结果显示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">第<span class="number">1</span>次: 梯度w.grad: <span class="number">1.000000</span>, 更新后的权重:<span class="number">0.990000</span> </span><br><span class="line">第<span class="number">2</span>次: 梯度w.grad: <span class="number">0.990000</span>, 更新后的权重:<span class="number">0.980003</span></span><br></pre></td></tr></table></figure>

<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><table>
<thead>
<tr>
<th><strong>优化算法</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>SGD</strong></td>
<td>简单、容易实现。</td>
<td>收敛速度较慢，容易震荡，特别是在复杂问题中。</td>
<td>用于简单任务，或者当数据特征分布相对稳定时。</td>
</tr>
<tr>
<td><strong>Momentum</strong></td>
<td>可以加速收敛，减少震荡，特别是在高曲率区域。</td>
<td>需要手动调整动量超参数，可能会在小步长训练中过度更新。</td>
<td>用于非平稳优化问题，尤其是深度学习中的应用。</td>
</tr>
<tr>
<td><strong>AdaGrad</strong></td>
<td>自适应调整学习率，适用于稀疏数据。</td>
<td>学习率会在训练过程中逐渐衰减，可能导致早期停滞。</td>
<td>适合稀疏数据，如 NLP 或推荐系统中的特征。</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>解决了 AdaGrad 学习率过早衰减的问题，适应性强。</td>
<td>需要选择合适的超参数，更新可能会过于激进。</td>
<td>适用于动态问题、非平稳目标函数，如深度学习训练。</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>结合了 Momentum 和 RMSProp 的优点，适应性强且稳定。</td>
<td>需要调节更多的超参数，训练过程中可能会产生较大波动。</td>
<td>广泛适用于各种深度学习任务，特别是非平稳和复杂问题。</td>
</tr>
</tbody></table>
<ul>
<li><p><strong>简单任务和较小的模型</strong>：SGD 或 Momentum</p>
</li>
<li><p><strong>复杂任务或有大量数据</strong>：Adam 是最常用的选择，因其在大部分任务上都表现优秀</p>
</li>
<li><p><strong>需要处理稀疏数据或文本数据</strong>：Adagrad 或 RMSProp</p>
</li>
</ul>
<h2 id="学习率衰减优化方法"><a href="#学习率衰减优化方法" class="headerlink" title="学习率衰减优化方法"></a>学习率衰减优化方法</h2><h3 id="为什么要进行学习率优化"><a href="#为什么要进行学习率优化" class="headerlink" title="为什么要进行学习率优化"></a>为什么要进行学习率优化</h3><p>在训练神经网络时，一般情况下学习率都会随着训练而变化。这主要是由于，在神经网络训练的后期，如果<strong>学习率过高，会造成loss的振荡</strong>，但是如果<strong>学习率减小的过慢，又会造成收敛变慢</strong>的情况。</p>
<p>运行下面代码，观察学习率设置不同对网络训练的影响：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x看成是权重，y看成是loss，下面通过代码来理解学习率的作用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">x_t</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">pow</span>(<span class="number">2</span>*x_t, <span class="number">2</span>)  <span class="comment"># y = 4 x ^2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用较小的学习率，梯度下降的速度慢</span></span><br><span class="line"><span class="comment"># 采用较大的学习率，梯度下降太快越过了最小值点，导致不收敛，甚至震荡</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line"></span><br><span class="line">    x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 记录loss迭代次数，画曲线</span></span><br><span class="line">    iter_rec, loss_rec, x_rec = <span class="built_in">list</span>(), <span class="built_in">list</span>(), <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实验学习率： 0.01 0.02 0.03 0.1 0.2 0.3 0.4</span></span><br><span class="line">    <span class="comment"># lr = 0.1    # 正常的梯度下降</span></span><br><span class="line">    <span class="comment"># lr = 0.125      # 当学习率设置0.125 一下子求出一个最优解</span></span><br><span class="line">                    <span class="comment"># x=0 y=0 在x=0处梯度等于0 x的值x=x-lr*x.grad就不用更新了</span></span><br><span class="line">                    <span class="comment"># 后续再多少次迭代 都固定在最优点</span></span><br><span class="line"></span><br><span class="line">    lr = <span class="number">0.2</span>      <span class="comment"># x从2.0一下子跨过0点，到了左侧负数区域</span></span><br><span class="line">    <span class="comment"># lr = 0.3      # 梯度越来越大 梯度爆炸</span></span><br><span class="line">    max_iteration = <span class="number">4</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iteration):</span><br><span class="line">        y = func(x)   <span class="comment"># 得出loss值</span></span><br><span class="line">        y.backward()  <span class="comment"># 计算x的梯度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Iter:&#123;&#125;, X:&#123;:8&#125;, X.grad:&#123;:8&#125;, loss:&#123;:10&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            i, x.detach().numpy()[<span class="number">0</span>], x.grad.detach().numpy()[<span class="number">0</span>], y.item()))</span><br><span class="line">        x_rec.append(x.item())      <span class="comment"># 梯度下降点 列表</span></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        x.data.sub_(lr * x.grad)    <span class="comment"># x = x - x.grad</span></span><br><span class="line">        x.grad.zero_()</span><br><span class="line">        iter_rec.append(i)          <span class="comment"># 迭代次数 列表</span></span><br><span class="line">        loss_rec.append(y)          <span class="comment"># 损失值 列表</span></span><br><span class="line">    <span class="comment"># 迭代次数-损失值 关系图</span></span><br><span class="line">    plt.subplot(<span class="number">121</span>).plot(iter_rec, loss_rec, <span class="string">&#x27;-ro&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Iteration X&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Loss value Y&quot;</span>)</span><br><span class="line">    <span class="comment"># 函数曲线-下降轨迹 显示图</span></span><br><span class="line">    x_t = torch.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">    y = func(x_t)</span><br><span class="line">    plt.subplot(<span class="number">122</span>).plot(x_t.numpy(), y.numpy(), label=<span class="string">&quot;y = 4*x^2&quot;</span>)</span><br><span class="line">    y_rec = [func(torch.tensor(i)).item() <span class="keyword">for</span> i <span class="keyword">in</span> x_rec]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;x_rec---&gt;&#x27;</span>, x_rec)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;y_rec---&gt;&#x27;</span>, y_rec)</span><br><span class="line">    <span class="comment"># 指定线的颜色和样式（-ro：红色圆圈，b-：蓝色实线等）</span></span><br><span class="line">    plt.subplot(<span class="number">122</span>).plot(x_rec, y_rec, <span class="string">&#x27;-ro&#x27;</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p><strong>运行效果图如下：</strong></p>
<p>可以看出：采用较小的学习率，梯度下降的速度慢；采用较大的学习率，梯度下降太快越过了最小值点，导致震荡，甚至不收敛（梯度爆炸）。</p>
<p><img src="/images/NN_base/04-01.png" alt="image-20220313172050673"></p>
<h3 id="等间隔学习率衰减"><a href="#等间隔学习率衰减" class="headerlink" title="等间隔学习率衰减"></a>等间隔学习率衰减</h3><p>等间隔学习率衰减方式如下所示：</p>
<p><img src="/images/NN_base/04-02.png" alt=" "></p>
<p>在PyTorch中实现时使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#   step_size：调整间隔数=50</span></span><br><span class="line"><span class="comment">#   gamma：调整系数=0.5</span></span><br><span class="line"><span class="comment">#   调整方式：lr = lr * gamma</span></span><br><span class="line">optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p>具体使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_StepLR</span>():</span><br><span class="line">    <span class="comment"># 0.参数初始化</span></span><br><span class="line">    LR = <span class="number">0.1</span>  <span class="comment"># 设置学习率初始化值为0.1</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    max_epoch = <span class="number">200</span></span><br><span class="line">    <span class="comment"># 1 初始化参数</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>])</span><br><span class="line">    x = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 2.优化器</span></span><br><span class="line">    optimizer = optim.SGD([w], lr=LR, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 3.设置学习率下降策略</span></span><br><span class="line">    scheduler_lr = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">50</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># 4.获取学习率的值和当前的epoch</span></span><br><span class="line">    lr_list, epoch_list = [], []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        lr_list.append(scheduler_lr.get_last_lr()) <span class="comment"># 获取当前lr</span></span><br><span class="line">        epoch_list.append(epoch) <span class="comment"># 获取当前的epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):  <span class="comment"># 遍历每一个batch数据</span></span><br><span class="line">            loss = (w*x-y_true)**<span class="number">2</span>  <span class="comment"># 目标函数</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        <span class="comment"># 更新下一个epoch的学习率</span></span><br><span class="line">        scheduler_lr.step()</span><br><span class="line">    <span class="comment"># 5.绘制学习率变化的曲线</span></span><br><span class="line">    plt.plot(epoch_list, lr_list, label=<span class="string">&quot;Step LR Scheduler&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()	</span><br></pre></td></tr></table></figure>

<h3 id="指定间隔学习率衰减"><a href="#指定间隔学习率衰减" class="headerlink" title="指定间隔学习率衰减"></a>指定间隔学习率衰减</h3><p>指定间隔学习率衰减的效果如下：</p>
<p><img src="/images/NN_base/04-03.png" alt=" "></p>
<p>在PyTorch中实现时使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># milestones：设定调整轮次:[50, 125, 160]</span></span><br><span class="line"><span class="comment"># gamma：调整系数</span></span><br><span class="line"><span class="comment"># 调整方式：lr = lr * gamma</span></span><br><span class="line">optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=<span class="number">0.1</span>, last_epoch=-<span class="number">1</span>)    </span><br></pre></td></tr></table></figure>

<p>具体使用方式如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_MultiStepLR</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">1</span>)</span><br><span class="line">    LR = <span class="number">0.1</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    max_epoch = <span class="number">200</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>])</span><br><span class="line">    x = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    optimizer = optim.SGD([w], lr=LR, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 设定调整时刻数</span></span><br><span class="line">    milestones = [<span class="number">50</span>, <span class="number">125</span>, <span class="number">160</span>]</span><br><span class="line">    <span class="comment"># 设置学习率下降策略</span></span><br><span class="line">    scheduler_lr = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=<span class="number">0.5</span>)</span><br><span class="line">    lr_list, epoch_list = <span class="built_in">list</span>(), <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        lr_list.append(scheduler_lr.get_last_lr())</span><br><span class="line">        epoch_list.append(epoch)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">            loss = (w*x-y_true)**<span class="number">2</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 参数更新</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">        <span class="comment"># 更新下一个epoch的学习率</span></span><br><span class="line">        scheduler_lr.step()</span><br><span class="line">    plt.plot(epoch_list, lr_list, label=<span class="string">&quot;Multi Step LR Scheduler\nmilestones:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(milestones))</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="按指数学习率衰减"><a href="#按指数学习率衰减" class="headerlink" title="按指数学习率衰减"></a>按指数学习率衰减</h3><p>按指数衰减调整学习率的效果如下：</p>
<p><img src="/images/NN_base/04-04.png" alt=" "></p>
<p>在PyTorch中实现时使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gamma：指数的底</span></span><br><span class="line"><span class="comment"># 调整方式</span></span><br><span class="line"><span class="comment"># lr= lr∗gamma^epoch</span></span><br><span class="line">optim.lr_scheduler.ExponentialLR(optimizer, gamma)</span><br></pre></td></tr></table></figure>

<p>具体使用方式如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_ExponentialLR</span>():</span><br><span class="line">    <span class="comment"># 0.参数初始化</span></span><br><span class="line">    LR = <span class="number">0.1</span>  <span class="comment"># 设置学习率初始化值为0.1</span></span><br><span class="line">    iteration = <span class="number">10</span></span><br><span class="line">    max_epoch = <span class="number">200</span></span><br><span class="line">    <span class="comment"># 1 初始化参数</span></span><br><span class="line">    y_true = torch.tensor([<span class="number">0</span>])</span><br><span class="line">    x = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">    w = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 2.优化器</span></span><br><span class="line">    optimizer = optim.SGD([w], lr=LR, momentum=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 3.设置学习率下降策略</span></span><br><span class="line">    gamma = <span class="number">0.95</span></span><br><span class="line">    scheduler_lr = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)</span><br><span class="line">    <span class="comment"># 4.获取学习率的值和当前的epoch</span></span><br><span class="line">    lr_list, epoch_list = <span class="built_in">list</span>(), <span class="built_in">list</span>()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_epoch):</span><br><span class="line">        lr_list.append(scheduler_lr.get_last_lr())</span><br><span class="line">        epoch_list.append(epoch)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iteration):  <span class="comment"># 遍历每一个batch数据</span></span><br><span class="line">            loss = (w*x-y_true)**<span class="number">2</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        <span class="comment"># 更新下一个epoch的学习率</span></span><br><span class="line">        scheduler_lr.step()</span><br><span class="line">    <span class="comment"># 5.绘制学习率变化的曲线</span></span><br><span class="line">    plt.plot(epoch_list, lr_list, label=<span class="string">&quot;Multi Step LR Scheduler&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Learning rate&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><table>
<thead>
<tr>
<th>方法</th>
<th>等间隔学习率衰减 (Step Decay)</th>
<th>指定间隔学习率衰减 (Exponential Decay)</th>
<th>指数学习率衰减 (Exponential Moving Average Decay)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>衰减方式</strong></td>
<td>固定步长衰减</td>
<td>指定步长衰减</td>
<td>平滑指数衰减，历史平均考虑</td>
</tr>
<tr>
<td><strong>实现难度</strong></td>
<td>简单易实现</td>
<td>相对简单，容易调整</td>
<td>需要额外历史计算，较复杂</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>大型数据集、较为简单的任务</td>
<td>对训练平稳性要求较高的任务</td>
<td>高精度训练，避免过快收敛</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>直观，易于调试，适用于大批量数据</td>
<td>易于调试，稳定训练过程</td>
<td>平滑且考虑历史更新，收敛稳定性较强</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>学习率变化较大，可能跳过最优点</td>
<td>在某些情况下可能衰减过快，导致优化提前停滞</td>
<td>超参数调节较为复杂，可能需要更多的计算资源</td>
</tr>
</tbody></table>
<h2 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h2><h3 id="什么是正则化"><a href="#什么是正则化" class="headerlink" title="什么是正则化"></a>什么是正则化</h3><p><img src="/images/NN_base/05-01.png" alt="05-01"></p>
<ul>
<li>在设计机器学习算法时希望在新样本上的泛化能力强。许多机器学习算法都采用相关的策略来减小测试误差，这些策略被统称为正则化</li>
<li>神经网络强大的表示能力经常遇到过拟合，所以需要使用不同形式的正则化策略</li>
<li>目前在深度学习中使用较多的策略有范数惩罚，DropOut，特殊的网络层等，接下来我们对其进行详细的介绍</li>
</ul>
<h3 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h3><p>在训练深层神经网络时，由于模型参数较多，在数据量不足的情况下，很容易过拟合。Dropout（中文翻译成随机失活）是一个简单有效的正则化方法。</p>
<p><img src="/images/NN_base/image-20220517141847967.png" alt="image-20220517141847967"></p>
<ul>
<li>在训练过程中，Dropout的实现是<strong>让神经元以超参数p（丢弃概率）的概率停止工作或者激活被置为0,未被置为0的进行缩放，缩放比例为1&#x2F;(1-p)</strong>。训练过程可以认为是对完整的神经网络的一些子集进行训练，每次基于输入数据只更新子网络的参数</li>
<li>在实际应用中，Dropout参数p的概率通常取值在0.2到0.5之间<ul>
<li>对于较小的模型或较复杂的任务，丢弃率可以选择0.3或更小</li>
<li>对于非常深的网络，较大的丢弃率（如0.5或0.6）可能会有效防止过拟合</li>
<li>实际应用中，通常会在全连接层（激活函数后）之后添加Dropout层</li>
</ul>
</li>
<li><strong>在测试过程中，随机失活不起作用</strong><ul>
<li>在测试阶段，使用所有的神经元进行预测，以获得更稳定的结果</li>
<li>直接使用训练好的模型进行测试，由于所有的神经元都参与计算，输出的期望值会比训练阶段高。测试阶段的期望输出是 E[x_test] &#x3D; x</li>
<li>测试&#x2F;推理模式：&#x3D;&#x3D;model.eval()&#x3D;&#x3D;</li>
</ul>
</li>
<li><strong>缩放的必要性</strong><ul>
<li>在训练阶段，将参与计算的神经元的输出除以(1-p)</li>
<li>经过Dropout后的期望输出变为 E[x_dropout] &#x3D; [(1-p) * x] &#x2F; (1-p) &#x3D; x，与测试阶段的期望输出一致</li>
<li>训练模型：&#x3D;&#x3D;model.train()&#x3D;&#x3D;</li>
</ul>
</li>
</ul>
<p>我们通过一段代码观察下dropout的效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    <span class="comment"># 初始化随机失活层</span></span><br><span class="line">    dropout = nn.Dropout(p=<span class="number">0.4</span>)</span><br><span class="line">    <span class="comment"># 初始化输入数据:表示某一层的weight信息</span></span><br><span class="line">    inputs = torch.randint(<span class="number">0</span>, <span class="number">10</span>, size=[<span class="number">1</span>, <span class="number">4</span>]).<span class="built_in">float</span>()</span><br><span class="line">    layer = nn.Linear(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">    y = layer(inputs)</span><br><span class="line">	y = torch.relu(y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;未失活FC层的输出结果：\n&quot;</span>, y)</span><br><span class="line">    y =  dropout(y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;失活后FC层的输出结果：\n&quot;</span>, y)</span><br></pre></td></tr></table></figure>

<p><strong>输出结果：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">未失活FC层的输出结果：</span><br><span class="line"> tensor([[<span class="number">0.0000</span>, <span class="number">1.8033</span>, <span class="number">1.4608</span>, <span class="number">4.5189</span>, <span class="number">6.9116</span>]], grad_fn=&lt;ReluBackward0&gt;)</span><br><span class="line">失活后FC层的输出结果：</span><br><span class="line"> tensor([[<span class="number">0.0000</span>,  <span class="number">3.0055</span>,  <span class="number">2.4346</span>,  <span class="number">7.5315</span>, <span class="number">11.5193</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>上述代码将Dropout层的丢弃概率p设置为0.4，此时经过Dropout层计算的张量中就出现了很多0, 未变为0的按照（1&#x2F;(1-0.4)）进行处理。</p>
<h3 id="批量归一正则化-Batch-Normalization"><a href="#批量归一正则化-Batch-Normalization" class="headerlink" title="批量归一正则化(Batch Normalization)"></a>批量归一正则化(Batch Normalization)</h3><p>在神经网络的训练过程中，流经网络的数据都是一个batch，每个batch之间的数据分布变化非常剧烈，&#x3D;&#x3D;这就使得网络参数频繁的进行大的调整以适应流经网络的不同分布的数据&#x3D;&#x3D;，给模型训练带来非常大的不稳定性，使得模型难以收敛。<strong>如果我们对每一个batch的数据进行标准化之后，数据分布就变得稳定，参数的梯度变化也变得稳定，有助于加快模型的收敛。</strong></p>
<p><strong>通过标准化每一层的输入，使其均值接近0，方差接近1，从而加速训练并提高泛化能力。</strong></p>
<p><img src="/images/NN_base/image-20220517143935599.png" alt="image-20220517143935599"></p>
<p>先对数据标准化，再对数据重构（缩放+平移），写成公式如下所示：</p>
<p><img src="/images/NN_base/48.png"></p>
<p>λ和β是可学习的参数，它相当于对标准化后的值做了一个<strong>线性变换</strong>，<strong>λ为系数，β为偏置；</strong></p>
<p>eps 通常指为 1e-5，避免分母为 0；</p>
<p>E(x) 表示变量的均值；</p>
<p>Var(x) 表示变量的方差；</p>
<p><strong>批量归一化的步骤如下：</strong></p>
<ol>
<li><p><strong>计算均值和方差</strong>：对于每个神经元（即每一层的输入特征），计算该特征在一个小批量（batch）上的均值 $$μ_B$$ 和方差 $$\sigma_B^2$$，它们的计算公式如下：</p>
<p>​            $$μ_B&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m} x_i​$$</p>
<p>​            $$σ_B^2&#x3D;\frac{1}{m} \sum_{i&#x3D;1}^{m} (x_i - \mu_B)^2$$</p>
<p>其中 $$x_i​$$ 表示小批量中的第 $$i​$$ 个样本，$$m​$$ 是小批量的样本数量。</p>
</li>
<li><p><strong>标准化</strong>：然后，对每个样本的输入进行标准化，得到归一化的输出：</p>
<p>​            $$\hat{x}_i &#x3D; \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}​$$</p>
<p>其中，$$ϵ$$ 是一个小常数，用来避免除以零的情况。</p>
</li>
<li><p><strong>缩放和平移</strong>：为了让网络能够恢复其学习能力，BN 层引入了两个可训练的参数 $$γ$$ 和 $$β$$，分别用于<strong>缩放</strong>和<strong>平移</strong>：</p>
<p>​            $$y_i &#x3D; \gamma \hat{x}_i + β$$</p>
<p>其中，$$γ$$ 和 $$β$$ 是可学习的参数，通过 γ 和 β，BN 层不再是简单的将每一层输入强行变为标准正态分布，而是允许网络学习更适合于该层的输入分布；规范化操作会丢失原始输入的一些信息，而 $$γ$$ 和 $$β$$ 可以弥补这种信息损失。</p>
</li>
</ol>
<p><strong>批量归一化的作用：</strong></p>
<ul>
<li><p>**减少内部协方差偏移：**通过对每层的输入进行标准化，减少了输入数据分布的变化，从而加速了训练过程，并使得网络在训练过程中更加稳定。</p>
</li>
<li><p><strong>加速训练：</strong></p>
<ul>
<li>在没有批量归一化的情况下，神经网络的训练通常会很慢，尤其是深度网络。因为在每层的训练过程中，输入数据的分布（特别是前几层）会不断变化，这会导致网络学习速度缓慢。</li>
<li>批量归一化通过确保每层的输入数据在训练时分布稳定，有效减少了这种变化，从而加速了训练过程。</li>
</ul>
</li>
<li><p>**起到正则化作用：**批量归一化可以视作一种正则化方法，因为它引入了对训练样本的噪声（不同批次的统计信息不同，批次较小的均值和方差估计会更加不准确），使得模型不容易依赖特定的输入特征，从而起到一定的正则化效果，减少了对其他正则化技术（如Dropout）的需求。</p>
</li>
<li><p>**提升泛化能力：**由于其正则化效果，批量归一化能帮助网络在测试集上取得更好的性能。</p>
</li>
</ul>
<p>&#x3D;&#x3D;<strong>批量归一化层在计算机视觉领域使用较多</strong>&#x3D;&#x3D;</p>
<p><strong>Batch Normalization 的使用步骤：</strong></p>
<ol>
<li><strong>在网络层后添加 BN 层：</strong><ul>
<li>通常，BN 层会添加在卷积层 (Conv2d) 或全连接层 (Linear) 之后，<strong>激活函数之前</strong>。</li>
<li>例如：Conv2d -&gt; BN -&gt; ReLU 或者 Linear -&gt; BN -&gt; ReLU。</li>
</ul>
</li>
<li><strong>训练时：</strong>&#x3D;&#x3D;model.train()&#x3D;&#x3D;<ul>
<li>BN 层会计算当前批次的均值 $$μ$$ 和方差 $$σ²$$。</li>
<li>然后，利用这两个统计量对当前批次的数据进行规范化。</li>
<li>规范化后的数据会被缩放 $$γ$$ 和平移 $$β$$。</li>
<li>同时，BN 层还会维护一个<strong>全局均值</strong>和<strong>全局方差</strong>的移动平均值，用于推理阶段。</li>
</ul>
</li>
<li><strong>推理时：</strong>&#x3D;&#x3D;model.eval()&#x3D;&#x3D;<ul>
<li>推理时，不会再使用当前批次的均值和方差，而是使用训练阶段计算的<strong>全局均值</strong>和<strong>全局方差</strong>。</li>
<li>同样，规范化后的数据会被缩放 $$γ$$ 和平移 $$β$$。</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">BatchNorm1d：主要应用于全连接层或处理一维数据的网络，例如文本处理。它接收形状为 (N, num_features) 的张量作为输入。</span></span><br><span class="line"><span class="string">BatchNorm2d：主要应用于卷积神经网络，处理二维图像数据或特征图。它接收形状为 (N, C, H, W) 的张量作为输入。</span></span><br><span class="line"><span class="string">BatchNorm3d：主要用于三维卷积神经网络 (3D CNN)，处理三维数据，例如视频或医学图像。它接收形状为 (N, C, D, H, W) 的张量作为输入。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tes01</span>():</span><br><span class="line">    <span class="comment"># 创建测试样本, 假设是经过卷积层(Conv2d)处理后的特征图</span></span><br><span class="line">    <span class="comment"># (N, C, H, W): 一张图, 两个通道, 每个通道3行4列</span></span><br><span class="line">    <span class="comment"># 可以创建1个样本, 图像的BN是对每个通道的特征图(行列数据)进行标准化</span></span><br><span class="line">    input_2d = torch.randn(size=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input--&gt;&quot;</span>, input_2d)</span><br><span class="line">    <span class="comment"># num_features：输入特征数</span></span><br><span class="line">    <span class="comment"># eps：非常小的浮点数，防止除以0的错误</span></span><br><span class="line">    <span class="comment"># momentum：动量系数</span></span><br><span class="line">    <span class="comment"># affine：默认为True，γ和β被使用，让BN层更加灵活</span></span><br><span class="line">    bn2d = nn.BatchNorm2d(num_features=<span class="number">2</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="literal">True</span>) </span><br><span class="line">    output = bn2d(input_2d)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output--&gt;&quot;</span>, output)</span><br><span class="line">    <span class="built_in">print</span>(output.size())</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(bn2d.weight)</span><br><span class="line">    <span class="built_in">print</span>(bn2d.bias)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tes02</span>():</span><br><span class="line">    <span class="comment"># 创建测试样本</span></span><br><span class="line">    <span class="comment"># 2个样本, 1个特征</span></span><br><span class="line">    <span class="comment"># 不能创建1个样本, 无法统计均值和方差</span></span><br><span class="line">    input_1d = torch.randn(size=(<span class="number">2</span>, <span class="number">2</span>)) </span><br><span class="line">    <span class="comment"># 创建线性层对象</span></span><br><span class="line">    linear1 = nn.Linear(in_features=<span class="number">2</span>, out_features=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 创建BN层对象</span></span><br><span class="line">    <span class="comment"># num_features：输入特征数</span></span><br><span class="line">	bn1d = nn.BatchNorm1d(num_features=<span class="number">3</span>)  <span class="comment"># 20 output features</span></span><br><span class="line">	output_1d = linear1(input_1d)</span><br><span class="line">    <span class="comment"># 进行批量归一化</span></span><br><span class="line">	output = bn1d(output_1d) </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;output--&gt;&quot;</span>, output)</span><br><span class="line">    <span class="built_in">print</span>(output.size()) <span class="comment"># (32, 20)</span></span><br></pre></td></tr></table></figure>

<p><strong>输出结果：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">test01:</span><br><span class="line">input_2d--&gt; tensor([[[[-<span class="number">0.2751</span>, -<span class="number">1.2183</span>, -<span class="number">0.5106</span>, -<span class="number">0.1540</span>],</span><br><span class="line">          [-<span class="number">0.4585</span>, -<span class="number">0.5989</span>, -<span class="number">0.6063</span>,  <span class="number">0.5986</span>],</span><br><span class="line">          [-<span class="number">0.4745</span>,  <span class="number">0.1496</span>, -<span class="number">1.1266</span>, -<span class="number">1.2377</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.2580</span>,  <span class="number">1.2065</span>,  <span class="number">1.4598</span>,  <span class="number">0.8387</span>],</span><br><span class="line">          [-<span class="number">0.4586</span>,  <span class="number">0.8938</span>, -<span class="number">0.3328</span>,  <span class="number">0.1192</span>],</span><br><span class="line">          [-<span class="number">0.3265</span>, -<span class="number">0.6263</span>,  <span class="number">0.0419</span>, -<span class="number">1.2231</span>]]]])</span><br><span class="line">output--&gt; tensor([[[[ <span class="number">0.4164</span>, -<span class="number">1.3889</span>, -<span class="number">0.0343</span>,  <span class="number">0.6484</span>],</span><br><span class="line">          [ <span class="number">0.0655</span>, -<span class="number">0.2032</span>, -<span class="number">0.2175</span>,  <span class="number">2.0889</span>],</span><br><span class="line">          [ <span class="number">0.0349</span>,  <span class="number">1.2294</span>, -<span class="number">1.2134</span>, -<span class="number">1.4262</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.1340</span>,  <span class="number">1.3582</span>,  <span class="number">1.6853</span>,  <span class="number">0.8835</span>],</span><br><span class="line">          [-<span class="number">0.7910</span>,  <span class="number">0.9546</span>, -<span class="number">0.6287</span>, -<span class="number">0.0452</span>],</span><br><span class="line">          [-<span class="number">0.6205</span>, -<span class="number">1.0075</span>, -<span class="number">0.1449</span>, -<span class="number">1.7779</span>]]]],</span><br><span class="line">       grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test02:</span><br><span class="line">output--&gt; tensor([[-<span class="number">0.9998</span>,  <span class="number">1.0000</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">0.9998</span>, -<span class="number">1.0000</span>, -<span class="number">1.0000</span>]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<h2 id="手机价格分类案例"><a href="#手机价格分类案例" class="headerlink" title="手机价格分类案例"></a>手机价格分类案例</h2><h3 id="案例需求分析"><a href="#案例需求分析" class="headerlink" title="案例需求分析"></a>案例需求分析</h3><p>小明创办了一家手机公司，他不知道如何估算手机产品的价格。为了解决这个问题，他收集了多家公司的手机销售数据。该数据为二手手机的各个性能的数据，最后根据这些性能得到4个价格区间，作为这些二手手机售出的价格区间。主要包括：</p>
<p><img src="/images/NN_base/1734255905857.png" alt="1734255905857"></p>
<p>我们需要帮助小明找出手机的功能（例如：RAM等）与其售价之间的某种关系。我们可以使用机器学习的方法来解决这个问题，也可以构建一个全连接的网络。<br>需要注意的是: 在这个问题中，我们不需要预测实际价格，而是一个价格范围，它的范围使用 0、1、2、3 来表示，所以该问题也是一个分类问题。接下来我们还是按照四个步骤来完成这个任务：</p>
<ul>
<li>准备训练集数据</li>
<li>构建要使用的模型</li>
<li>模型训练</li>
<li>模型预测评估</li>
</ul>
<h3 id="构建数据集"><a href="#构建数据集" class="headerlink" title="构建数据集"></a>构建数据集</h3><p>数据共有 2000 条, 其中 1600 条数据作为训练集, 400 条数据用作测试集。 我们使用 sklearn 的数据集划分工作来完成。并使用 PyTorch 的 TensorDataset 来将数据集构建为 Dataset 对象，方便构造数据集加载对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入相关模块</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">    <span class="comment"># 使用pandas读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&#x27;data/手机价格预测.csv&#x27;</span>)</span><br><span class="line">    <span class="comment"># 特征值和目标值</span></span><br><span class="line">    x, y = data.iloc[:, :-<span class="number">1</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 类型转换：特征值</span></span><br><span class="line">    x = x.astype(np.float32)</span><br><span class="line">    <span class="comment"># 数据集划分</span></span><br><span class="line">    x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=<span class="number">0.8</span>, random_state=<span class="number">88</span>)</span><br><span class="line">    <span class="comment"># 构建数据集,转换为pytorch的形式</span></span><br><span class="line">    train_dataset = TensorDataset(torch.from_numpy(x_train.values), torch.tensor(y_train.values))</span><br><span class="line">    valid_dataset = TensorDataset(torch.from_numpy(x_valid.values), torch.tensor(y_valid.values))</span><br><span class="line">    <span class="comment"># 返回结果</span></span><br><span class="line">    <span class="comment"># x_train.shape[1]: 特征数</span></span><br><span class="line">    <span class="comment"># len(np.unique(y)): 类别数</span></span><br><span class="line">    <span class="keyword">return</span> train_dataset, valid_dataset, x_train.shape[<span class="number">1</span>], <span class="built_in">len</span>(np.unique(y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    train_dataset, valid_dataset, input_dim, class_num = create_dataset()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;输入特征数：&quot;</span>, input_dim)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;分类个数：&quot;</span>, class_num)</span><br></pre></td></tr></table></figure>

<p>输出结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入特征数： <span class="number">20</span> </span><br><span class="line">分类个数： <span class="number">4</span></span><br></pre></td></tr></table></figure>

<h3 id="构建分类网络模型"><a href="#构建分类网络模型" class="headerlink" title="构建分类网络模型"></a>构建分类网络模型</h3><p>构建全连接神经网络来进行手机价格分类，该网络主要由三个线性层来构建，使用relu激活函数。<br>网络共有 3 个全连接层, 具体信息如下:<br>第一层: 输入为维度为 20, 输出维度为: 128<br>第二层: 输入为维度为 128, 输出维度为: 256<br>第三层: 输入为维度为 256, 输出维度为: 4</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PhonePriceModel</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">		<span class="built_in">super</span>(PhonePriceModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">		<span class="comment"># 1. 第一层: 输入为维度为 20, 输出维度为: 128</span></span><br><span class="line">		<span class="variable language_">self</span>.linear1 = nn.Linear(input_dim, <span class="number">128</span>)</span><br><span class="line">		<span class="comment"># 2. 第二层: 输入为维度为 128, 输出维度为: 256</span></span><br><span class="line">		<span class="variable language_">self</span>.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">		<span class="comment"># 3. 第三层: 输入为维度为 256, 输出维度为: 4</span></span><br><span class="line">		<span class="variable language_">self</span>.linear3 = nn.Linear(<span class="number">256</span>, output_dim)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		<span class="comment"># 前向传播过程</span></span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear1(x))</span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear2(x))</span><br><span class="line">        <span class="comment"># 后续CrossEntropyLoss损失函数中包含softmax过程, 所以当前步骤不进行softmax操作</span></span><br><span class="line">		output = <span class="variable language_">self</span>.linear3(x)</span><br><span class="line">		<span class="comment"># 获取数据结果</span></span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_dataset, valid_dataset, input_dim, class_num = create_dataset()</span><br><span class="line">    <span class="comment"># 模型实例化</span></span><br><span class="line">    model = PhonePriceModel(input_dim, class_num)</span><br><span class="line">    summary(model, input_size=(input_dim,), batch_size=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/images/NN_base/1734256199295.png" alt="1734256199295"></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>网络编写完成之后，我们需要编写训练函数。所谓的训练函数，指的是输入数据读取、送入网络、计算损失、更新参数的流程，该流程较为固定。我们使用的是多分类交叉生损失函数、使用 SGD 优化方法。最终，将训练好的模型持久化到磁盘中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型训练过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dataset,input_dim,class_num</span>):</span><br><span class="line">    <span class="comment"># 固定随机数种子</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 初始化数据加载器</span></span><br><span class="line">    dataloader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>)</span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    model = PhonePriceModel(input_dim, class_num)</span><br><span class="line">    <span class="comment"># 损失函数  CrossEntropyLoss = softmax + 损失计算</span></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="comment"># 优化方法</span></span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    <span class="comment"># 训练轮数</span></span><br><span class="line">    num_epoch = <span class="number">50</span></span><br><span class="line">    <span class="comment"># 遍历每个轮次的数据</span></span><br><span class="line">    <span class="keyword">for</span> epoch_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">        <span class="comment"># 训练时间</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        total_loss = <span class="number">0.0</span></span><br><span class="line">        total_num = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 遍历每个batch数据进行处理</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            <span class="comment"># 将数据送入网络中进行预测</span></span><br><span class="line">            model.train() <span class="comment"># 使用训练模式</span></span><br><span class="line">            output = model(x)</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = criterion(output, y)</span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 参数更新</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># 损失计算</span></span><br><span class="line">            total_num += <span class="number">1</span></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">        <span class="comment"># 打印损失变换结果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: %4s loss: %.2f, time: %.2fs&#x27;</span> %(epoch_idx + <span class="number">1</span>, total_loss / total_num, time.time() - start))</span><br><span class="line">    <span class="comment"># 模型保存</span></span><br><span class="line">    <span class="comment"># state_dict(): 将模型的参数保存到字典中</span></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;model/phone-price-model.pth&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:    </span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    train_dataset, valid_dataset, input_dim, class_num = create_dataset()</span><br><span class="line">    <span class="comment"># 模型训练过程</span></span><br><span class="line">    train(train_dataset,input_dim,class_num)</span><br></pre></td></tr></table></figure>

<p><img src="/images/NN_base/1734256384162.png" alt="1734256384162"></p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>使用训练好的模型，对未知的样本的进行预测的过程。我们这里使用前面单独划分出来的验证集来进行评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">valid_dataset, input_dim, class_num</span>):</span><br><span class="line">    <span class="comment"># 加载模型和训练好的网络参数</span></span><br><span class="line">    model = PhonePriceModel(input_dim, class_num)</span><br><span class="line">    <span class="comment"># load_state_dict:将加载的参数字典应用到模型上</span></span><br><span class="line">	<span class="comment"># load:加载用来保存模型参数的文件</span></span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&#x27;model/phone-price-model.pth&#x27;</span>))</span><br><span class="line">    <span class="comment"># 构建加载器</span></span><br><span class="line">    dataloader = DataLoader(valid_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 评估测试集</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历测试集中的数据</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="comment"># 将其送入网络中</span></span><br><span class="line">        model.<span class="built_in">eval</span>() <span class="comment"># 使用推理模式 </span></span><br><span class="line">        output = model(x)</span><br><span class="line">        <span class="comment"># 获取类别结果</span></span><br><span class="line">        <span class="comment"># argmax: 最大值对应的下标, 即类别编码</span></span><br><span class="line">        y_pred = torch.argmax(output, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 获取预测正确的个数</span></span><br><span class="line">        correct += (y_pred == y).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="comment"># 求预测精度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Acc: %.5f&#x27;</span> % (correct.item() / <span class="built_in">len</span>(valid_dataset)))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    train_dataset, valid_dataset, input_dim, class_num = create_dataset()</span><br><span class="line">    <span class="comment"># 模型预测结果</span></span><br><span class="line">    test(valid_dataset, input_dim, class_num)</span><br></pre></td></tr></table></figure>

<p><strong>输出结果：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Acc: <span class="number">0.64250</span></span><br></pre></td></tr></table></figure>

<h3 id="网络性能优化"><a href="#网络性能优化" class="headerlink" title="网络性能优化"></a>网络性能优化</h3><p>我们前面的网络模型在测试集的准确率为: 0.64250, 我们可以通过以下方面进行调优:</p>
<ol>
<li>对输入数据进行标准化</li>
<li>调整优化方法</li>
<li>调整学习率</li>
<li>增加批量归一化层</li>
<li>增加网络层数、神经元个数</li>
<li>增加训练轮数</li>
<li>等等…</li>
</ol>
<p>进行下如下调整:</p>
<ol>
<li>优化方法由 SGD 调整为 Adam</li>
<li>学习率由 1e-3 调整为 1e-4</li>
<li>对数据进行标准化</li>
<li>增加网络深度, 即: 增加网络参数量</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>():</span><br><span class="line">	<span class="comment"># 使用pandas读取数据</span></span><br><span class="line">	data = pd.read_csv(<span class="string">&#x27;./data/手机价格预测.csv&#x27;</span>)</span><br><span class="line">	<span class="comment"># 特征值和目标值</span></span><br><span class="line">	x, y = data.iloc[:, :-<span class="number">1</span>], data.iloc[:, -<span class="number">1</span>]</span><br><span class="line">	<span class="comment"># 类型转换：特征值，目标值</span></span><br><span class="line">	x = x.astype(np.float32)</span><br><span class="line">	y = y.astype(np.int64)</span><br><span class="line">	<span class="comment"># 数据集划分</span></span><br><span class="line">	x_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=<span class="number">0.8</span>, random_state=<span class="number">88</span>, stratify=y)</span><br><span class="line">	<span class="comment"># 优化①:数据标准化</span></span><br><span class="line">	transfer = StandardScaler()</span><br><span class="line">	x_train = transfer.fit_transform(x_train)</span><br><span class="line">	x_valid = transfer.transform(x_valid)</span><br><span class="line">	<span class="comment"># 构建数据集,转换为pytorch的形式</span></span><br><span class="line">	train_dataset = TensorDataset(torch.from_numpy(x_train), torch.tensor(y_train.values))</span><br><span class="line">	valid_dataset = TensorDataset(torch.from_numpy(x_valid), torch.tensor(y_valid.values))</span><br><span class="line">	<span class="comment"># 返回结果</span></span><br><span class="line">	<span class="keyword">return</span> train_dataset, valid_dataset, x_train.shape[<span class="number">1</span>], <span class="built_in">len</span>(np.unique(y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PhonePriceModel</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">		<span class="built_in">super</span>(PhonePriceModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">		<span class="comment"># 优化②:增加网络深度</span></span><br><span class="line">		<span class="comment"># 1. 第一层: 输入为维度为 20, 输出维度为: 128</span></span><br><span class="line">		<span class="variable language_">self</span>.linear1 = nn.Linear(input_dim, <span class="number">128</span>)</span><br><span class="line">		<span class="comment"># 2. 第二层: 输入为维度为 128, 输出维度为: 256</span></span><br><span class="line">		<span class="variable language_">self</span>.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">		<span class="comment"># 3. 第三层: 输入为维度为 256, 输出维度为: 512</span></span><br><span class="line">		<span class="variable language_">self</span>.linear3 = nn.Linear(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">		<span class="comment"># 4. 第三层: 输入为维度为 512, 输出维度为: 128</span></span><br><span class="line">		<span class="variable language_">self</span>.linear4 = nn.Linear(<span class="number">512</span>, <span class="number">128</span>)</span><br><span class="line">		<span class="comment"># 5. 第三层: 输入为维度为 128, 输出维度为: 4</span></span><br><span class="line">		<span class="variable language_">self</span>.linear5 = nn.Linear(<span class="number">128</span>, output_dim)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		<span class="comment"># 前向传播过程</span></span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear1(x))</span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear2(x))</span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear3(x))</span><br><span class="line">		x = torch.relu(<span class="variable language_">self</span>.linear4(x))</span><br><span class="line">		<span class="comment"># 后续CrossEntropyLoss损失函数中包含softmax过程, 所以当前步骤不进行softmax操作</span></span><br><span class="line">		output = <span class="variable language_">self</span>.linear5(x)</span><br><span class="line">		<span class="comment"># 获取数据结果</span></span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编写训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_dataset, input_dim, class_num</span>):</span><br><span class="line">	<span class="comment"># 固定随机数种子</span></span><br><span class="line">	torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">	<span class="comment"># 初始化数据加载器</span></span><br><span class="line">	dataloader = DataLoader(train_dataset, shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>)</span><br><span class="line">	<span class="comment"># 初始化模型</span></span><br><span class="line">	model = PhonePriceModel(input_dim, class_num)</span><br><span class="line">	<span class="comment"># 损失函数 CrossEntropyLoss = softmax + 损失计算</span></span><br><span class="line">	criterion = nn.CrossEntropyLoss()</span><br><span class="line">	<span class="comment"># 优化③:使用Adam优化方法, 优化④:学习率变为1e-4</span></span><br><span class="line">	optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">	<span class="comment"># 遍历每个轮次的数据</span></span><br><span class="line">	num_epoch = <span class="number">50</span></span><br><span class="line">	<span class="keyword">for</span> epoch_idx <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">		<span class="comment"># 训练时间</span></span><br><span class="line">		start = time.time()</span><br><span class="line">		<span class="comment"># 计算损失</span></span><br><span class="line">		total_loss = <span class="number">0.0</span></span><br><span class="line">		total_num = <span class="number">0</span></span><br><span class="line">		<span class="comment"># 遍历每个batch数据进行处理</span></span><br><span class="line">		<span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">			model.train()</span><br><span class="line">			output = model(x)</span><br><span class="line">			<span class="comment"># 计算损失</span></span><br><span class="line">			loss = criterion(output, y)</span><br><span class="line">			<span class="comment"># 梯度清零</span></span><br><span class="line">			optimizer.zero_grad()</span><br><span class="line">			<span class="comment"># 反向传播</span></span><br><span class="line">			loss.backward()</span><br><span class="line">			<span class="comment"># 参数更新</span></span><br><span class="line">			optimizer.step()</span><br><span class="line">			<span class="comment"># 损失计算</span></span><br><span class="line">			total_num += <span class="built_in">len</span>(y)</span><br><span class="line">			total_loss += loss.item() * <span class="built_in">len</span>(y)</span><br><span class="line">		<span class="comment"># 打印损失变换结果</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;epoch: %4s loss: %.2f, time: %.2fs&#x27;</span> %</span><br><span class="line">			  (epoch_idx + <span class="number">1</span>, total_loss / total_num, time.time() - start))</span><br><span class="line">	<span class="comment"># 模型保存</span></span><br><span class="line">	torch.save(model.state_dict(), <span class="string">&#x27;./model/phone-price-model2.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">valid_dataset, input_dim, class_num</span>):</span><br><span class="line">	<span class="comment"># 加载模型和训练好的网络参数</span></span><br><span class="line">	model = PhonePriceModel(input_dim, class_num)</span><br><span class="line">	<span class="comment"># load_state_dict:将加载的参数字典应用到模型上</span></span><br><span class="line">	<span class="comment"># load:加载用来保存模型参数的文件</span></span><br><span class="line">	model.load_state_dict(torch.load(<span class="string">&#x27;./model/phone-price-model2.pth&#x27;</span>))</span><br><span class="line">	<span class="comment"># 构建加载器</span></span><br><span class="line">	dataloader = DataLoader(valid_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">	<span class="comment"># 评估测试集</span></span><br><span class="line">	correct = <span class="number">0</span></span><br><span class="line">	<span class="comment"># 遍历测试集中的数据</span></span><br><span class="line">	<span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">		<span class="comment"># 将其送入网络中</span></span><br><span class="line">		<span class="comment"># model.eval()</span></span><br><span class="line">		output = model(x)</span><br><span class="line">		<span class="comment"># 获取预测类别结果</span></span><br><span class="line">		y_pred = torch.argmax(output, dim=<span class="number">1</span>)</span><br><span class="line">		<span class="comment"># 获取预测正确的个数</span></span><br><span class="line">		correct += (y_pred == y).<span class="built_in">sum</span>()</span><br><span class="line">	<span class="comment"># 求预测精度</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;Acc: %.5f&#x27;</span> % (correct / <span class="built_in">len</span>(valid_dataset)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	train_dataset, valid_dataset, input_dim, class_num = create_dataset()</span><br><span class="line">	train(train_dataset, input_dim, class_num)</span><br><span class="line">	test(valid_dataset, input_dim, class_num)</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/05/14/a-003-%E5%AE%9D%E5%85%B8%E9%9D%A2%E8%AF%95%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/" rel="prev" title="面试宝典（持续更新)">
                  <i class="fa fa-angle-left"></i> 面试宝典（持续更新)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/05/21/b-001-one-hot%E5%90%8D%E5%AD%97%E7%94%B1%E6%9D%A5/" rel="next" title="one-hot名字由来">
                  one-hot名字由来 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">李响</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
